<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>underworld3.swarm API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML'></script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>underworld3.swarm</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Optional, Tuple
import contextlib

import numpy as np
import sympy
import petsc4py.PETSc as PETSc
from mpi4py import MPI

import underworld3 as uw
from underworld3.utilities import _api_tools
import underworld3.timing as timing

import h5py
import os
import warnings

comm = MPI.COMM_WORLD

from enum import Enum


class SwarmType(Enum):
    DMSWARM_PIC = 1


class SwarmPICLayout(Enum):
    &#34;&#34;&#34;
    Particle population fill type:

    SwarmPICLayout.REGULAR     defines points on a regular ijk mesh. Supported by simplex cell types only.
    SwarmPICLayout.GAUSS       defines points using an npoint Gauss-Legendre tensor product quadrature rule.
    SwarmPICLayout.SUBDIVISION defines points on the centroid of a sub-divided reference cell.
    &#34;&#34;&#34;

    REGULAR = 0
    GAUSS = 1
    SUBDIVISION = 2


# Note - much of the setup is necessarily the same as the MeshVariable
# and the duplication should be removed.


class SwarmVariable(_api_tools.Stateful):
    @timing.routine_timer_decorator
    def __init__(
        self,
        name,
        swarm,
        size,  # only needed if MATRIX type
        vtype=None,
        dtype=float,
        proxy_degree=1,
        proxy_continuous=True,
        _register=True,
        _proxy=True,
        _nn_proxy=False,
        varsymbol=None,
        rebuild_on_cycle=True,
    ):

        if name in swarm.vars.keys():
            raise ValueError(
                &#34;Variable with name {} already exists on swarm.&#34;.format(name)
            )

        import re
        import sympy
        import math

        if varsymbol is None:
            varsymbol = name

        self.name = name
        self.clean_name = re.sub(r&#34;[^a-zA-Z0-9_]&#34;, &#34;&#34;, name)
        self.symbol = varsymbol

        self.swarm = swarm
        self.shape = size

        mesh = swarm.mesh

        if vtype == None:
            if isinstance(size, int) and size == 1:
                vtype = uw.VarType.SCALAR
            elif isinstance(size, int) and size == mesh.dim:
                vtype = uw.VarType.VECTOR
            elif isinstance(size, tuple):
                if size[0] == mesh.dim and size[1] == mesh.dim:
                    vtype = uw.VarType.TENSOR
                else:
                    vtype = uw.VarType.MATRIX
            else:
                raise ValueError(
                    &#34;Unable to infer variable type from `num_components`. Please explicitly set the `vtype` parameter.&#34;
                )

        if not isinstance(vtype, uw.VarType):
            raise ValueError(
                &#34;&#39;vtype&#39; must be an instance of &#39;Variable_Type&#39;, for example `underworld.VarType.SCALAR`.&#34;
            )

        if vtype == uw.VarType.SCALAR:
            self.num_components = 1
            self.shape = 1
            self.cpt_map = 0
        elif vtype == uw.VarType.VECTOR:
            self.num_components = mesh.dim
            self.shape = mesh.dim
            self.cpt_map = tuple(range(0, mesh.dim))
        elif vtype == uw.VarType.TENSOR:
            self.num_components = mesh.dim * mesh.dim
            self.shape = (mesh.dim, mesh.dim)
        elif vtype == uw.VarType.SYM_TENSOR:
            self.num_components = math.comb(mesh.dim + 1, 2)
            self.shape = (mesh.dim, mesh.dim)
        elif vtype == uw.VarType.MATRIX:
            self.num_components = self.shape[0] * self.shape[1]

        if (dtype == float) or (dtype == &#34;float&#34;) or (dtype == np.float64):
            self.dtype = float
            petsc_type = PETSc.ScalarType
        elif (
            (dtype == int)
            or (dtype == &#34;int&#34;)
            or (dtype == np.int32)
            or (dtype == np.int64)
        ):
            self.dtype = int
            petsc_type = PETSc.IntType
        else:
            raise TypeError(
                f&#34;Provided dtype={dtype} is not supported. Supported types are &#39;int&#39; and &#39;float&#39;.&#34;
            )

        if _register:
            self.swarm.dm.registerField(
                self.clean_name, self.num_components, dtype=petsc_type
            )

        self._data = None
        # add to swarms dict

        self.swarm._vars[self.clean_name] = self
        self._is_accessed = False

        # proxy variable
        self._proxy = _proxy
        self._vtype = vtype
        self._proxy_degree = proxy_degree
        self._proxy_continuous = proxy_continuous
        self._nn_proxy = _nn_proxy
        self._create_proxy_variable()

        # recycle swarm
        self._rebuild_on_cycle = rebuild_on_cycle

        self._register = _register

        super().__init__()

        return

    def _create_proxy_variable(self):

        # release if defined
        self._meshVar = None

        if self._proxy:
            self._meshVar = uw.discretisation.MeshVariable(
                &#34;proxy_&#34; + self.clean_name,
                self.swarm._mesh,
                self.shape,
                self._vtype,
                degree=self._proxy_degree,
                continuous=self._proxy_continuous,
                varsymbol=r&#34;\left&lt;&#34; + self.symbol + r&#34;\right&gt;&#34;,
            )

    def _update(self):
        &#34;&#34;&#34;
        This method updates the proxy mesh variable for the current
        swarm &amp; particle variable state.
        &#34;&#34;&#34;

        # if not proxied, nothing to do. return.
        if not self._meshVar:
            return

        else:
            self._rbf_to_meshVar(self._meshVar)

        return

    # Maybe rbf_interpolate for this one and meshVar is a special case
    def _rbf_to_meshVar(self, meshVar, nnn=None, verbose=False):
        &#34;&#34;&#34;
        Here is how it works: for each particle, create a distance-weighted average on the node data

        Todo: caching the k-d trees etc for the proxy-mesh-variable nodal points
        Todo: some form of global fall-back for when there are no particles on a processor
        &#34;&#34;&#34;

        # Mapping to the coordinates of the variable from the
        # particle coords

        if nnn is None:
            nnn = self.swarm.mesh.dim + 1

        if meshVar.mesh != self.swarm.mesh:
            raise RuntimeError(&#34;Cannot map a swarm to a different mesh&#34;)

        new_coords = meshVar.coords

        Values = self.rbf_interpolate(new_coords, verbose=verbose, nnn=nnn)

        with meshVar.mesh.access(meshVar):
            meshVar.data[...] = Values[...]

        return

    def rbf_interpolate(self, new_coords, verbose=False, nnn=None):

        # An inverse-distance mapping is quite robust here ... as long
        # as long we take care of the case where some nodes coincide (likely if used mesh2mesh)
        # We try to eliminate contributions from recently remeshed particles

        import numpy as np

        if nnn is None:
            nnn = self.swarm.mesh.dim + 1

        with self.swarm.access():
            if self.swarm.recycle_rate &gt; 1:
                not_remeshed = self.swarm._remeshed.data[:, 0] != 0
                D = self.data[not_remeshed].copy()

                kdt = uw.kdtree.KDTree(
                    self.swarm.particle_coordinates.data[not_remeshed, :]
                )
            else:
                D = self.data.copy()
                kdt = uw.kdtree.KDTree(self.swarm.particle_coordinates.data[:, :])

            kdt.build_index()

        return kdt.rbf_interpolator_local(new_coords, D, nnn, verbose)

    # ToDo: I don&#39;t think this is used / up to date
    @timing.routine_timer_decorator
    def project_from(self, meshvar):
        # use method found in
        # /tmp/petsc-build/petsc/src/dm/impls/swarm/tests/ex2.c
        # to project from fields to particles

        self.swarm.mesh.dm.clearDS()
        self.swarm.mesh.dm.createDS()

        meshdm = meshvar.mesh.dm
        fields = meshvar.field_id
        _, meshvardm = meshdm.createSubDM(fields)

        ksp = PETSc.KSP().create()
        ksp.setOptionsPrefix(&#34;swarm_project_from_&#34;)
        options = PETSc.Options()
        options.setValue(&#34;swarm_project_from_ksp_type&#34;, &#34;lsqr&#34;)
        options.setValue(&#34;swarm_project_from_ksp_rtol&#34;, 1e-17)
        options.setValue(&#34;swarm_project_from_pc_type&#34;, &#34;none&#34;)
        ksp.setFromOptions()

        rhs = meshvardm.getGlobalVec()

        M_p = self.swarm.dm.createMassMatrix(meshvardm)

        # make particle weight vector
        f = self.swarm.createGlobalVectorFromField(self.clean_name)

        # create matrix RHS vector, in this case the FEM field fhat with the coefficients vector #alpha
        M = meshvardm.createMassMatrix(meshvardm)
        with meshvar.mesh.access():
            M.multTranspose(meshvar.vec_global, rhs)

        ksp.setOperators(M_p, M_p)
        ksp.solveTranspose(rhs, f)

        self.swarm.dm.destroyGlobalVectorFromField(self.clean_name)
        meshvardm.restoreGlobalVec(rhs)
        meshvardm.destroy()
        ksp.destroy()
        M.destroy()
        M_p.destroy()

    @property
    def data(self):
        if self._data is None:
            raise RuntimeError(
                &#34;Data must be accessed via the swarm `access()` context manager.&#34;
            )
        return self._data

    @property
    def sym(self):
        return self._meshVar.sym

    @property
    def sym_1d(self):
        return self._meshVar.sym_1d

    @timing.routine_timer_decorator
    def save(
        self,
        filename: int,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential=False,
    ):
        &#34;&#34;&#34;

        Save the swarm variable to a h5 file.

        Parameters
        ----------
        filename :
            The filename of the swarm variable to save to disk.
        compression :
            Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
        compressionType :
            Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.

        &#34;&#34;&#34;
        if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
            warnings.warn(
                &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
                stacklevel=2,
            )
        if compression == True and comm.rank == 0:
            warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)
        if filename.endswith(&#34;.h5&#34;) == False:
            raise RuntimeError(&#34;The filename must end with .h5&#34;)

        if h5py.h5.get_config().mpi == True and not force_sequential:
            with h5py.File(
                f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=MPI.COMM_WORLD
            ) as h5f:
                with self.swarm.access(self):
                    if compression == True:
                        h5f.create_dataset(
                            &#34;data&#34;, data=self.data[:], compression=compressionType
                        )
                    else:
                        h5f.create_dataset(&#34;data&#34;, data=self.data[:])
        else:
            with self.swarm.access(self):
                if comm.rank == 0:
                    # print(f&#39;start {self.name} on {comm.rank}&#39;)
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                        if compression == True:
                            h5f.create_dataset(
                                &#34;data&#34;,
                                data=self.data[:],
                                chunks=True,
                                maxshape=(None, self.data.shape[1]),
                                compression=compressionType,
                            )
                        else:
                            h5f.create_dataset(
                                &#34;data&#34;,
                                data=self.data[:],
                                chunks=True,
                                maxshape=(None, self.data.shape[1]),
                            )
                    # print(f&#39;finish {self.name} on {comm.rank}&#39;)
                comm.barrier()
                for proc in range(1, comm.size):
                    if comm.rank == proc:
                        # print(f&#39;start {self.name} on {comm.rank}&#39;)
                        with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                            h5f[&#34;data&#34;].resize(
                                (h5f[&#34;data&#34;].shape[0] + self.data.shape[0]), axis=0
                            )
                            h5f[&#34;data&#34;][-self.data.shape[0] :] = self.data[:]
                        # print(f&#39;finish {self.name} on {comm.rank}&#39;)
                    comm.barrier()
                comm.barrier()

        return

    @timing.routine_timer_decorator
    def simple_save(self, filename: str):

        # if not proxied, nothing to do. return.
        if not self._meshVar:
            if uw.mpi.rank == 0:
                print(&#34;No proxy mesh variable that can be saved&#34;, flush=True)
            return

        self._meshVar.simple_save(filename)

        return

    @timing.routine_timer_decorator
    def load(
        self,
        filename: str,
        swarmFilename: str,
    ):
        ### open up file with coords on all procs and open up data on all procs. May be problematic for large problems.
        with h5py.File(f&#34;{filename}&#34;, &#34;r&#34;) as h5f_data, h5py.File(
            f&#34;{swarmFilename}&#34;, &#34;r&#34;
        ) as h5f_swarm:
            with self.swarm.access(self):
                var_dtype = self.data.dtype
                file_dtype = h5f_data[&#34;data&#34;][:].dtype
                file_length = h5f_data[&#34;data&#34;][:].shape[0]

                if var_dtype != file_dtype:
                    if comm.rank == 0:
                        warnings.warn(
                            f&#34;{os.path.basename(filename)} dtype ({file_dtype}) does not match {self.name} swarm variable dtype ({var_dtype}) which may result in a loss of data.&#34;,
                            stacklevel=2,
                        )

                # First work out which are local points and ignore the rest
                # This might help speed up the load by dropping lots of particles

                all_coords = h5f_swarm[&#34;coordinates&#34;][()]
                all_data = h5f_data[&#34;data&#34;][()]

                cell = self.swarm.mesh.get_closest_local_cells(all_coords)
                local = np.where(cell &gt;= 0)[0]
                # not_not_local = np.where(cell == -1)[0]

                local_coords = all_coords[local]
                local_data = all_data[local]

                kdt = uw.kdtree.KDTree(local_coords)

                self.data[:] = kdt.rbf_interpolator_local(
                    self.swarm.data, local_data, nnn=1
                )

                #### this produces a shape mismatch, would be quicker not to do it in a loop
                # ind = np.isin(coordinates, self.swarm.data).all(axis=1)
                # # self.data[:] = data[ind]

                # print(f&#34;Looping over coords for swarm load&#34;)
                # i = 0

                ### loops through the coords of the swarm to load the data
                # for coord in self.swarm.data:
                #     ind_data = np.isin(h5f_swarm[&#34;coordinates&#34;][:], coord).all(axis=1)
                #     ind_swarm = np.isin(self.swarm.data, coord).all(axis=1)
                #     self.data[ind_swarm] = h5f_data[&#34;data&#34;][:][ind_data]
                #     i += 1
                #     if i % 1000 == 0:
                #         print(
                #             f&#34;Looping over coords for swarm load ... {i}/{self.swarm.data.shape[0]}&#34;
                #         )

                # print(f&#34;Looping over coords for swarm load ... done&#34;)

                ### loops through the coords in the file
                # for i in range(0, file_length):
                #     coord = h5f_swarm[&#34;coordinates&#34;][i]
                #     data = h5f_data[&#34;data&#34;][i]
                #     ind_swarm = np.isin(self.swarm.data, coord).all(axis=1)

                #     self.data[ind_swarm] = data

        return


class IndexSwarmVariable(SwarmVariable):
    &#34;&#34;&#34;
    The IndexSwarmVariable is a class for managing material point
    behaviour. The material index variable is rendered into a
    collection of masks each representing the extent of one material
    &#34;&#34;&#34;

    @timing.routine_timer_decorator
    def __init__(
        self,
        name,
        swarm,
        indices=1,
        proxy_degree=1,
        proxy_continuous=True,
    ):

        self.indices = indices

        # These are the things we require of the generic swarm variable type
        super().__init__(
            name,
            swarm,
            size=1,
            vtype=None,
            dtype=int,
            _proxy=False,
        )
        &#34;&#34;&#34;
        vtype = (None,)
        dtype = (float,)
        proxy_degree = (1,)
        proxy_continuous = (True,)
        _register = (True,)
        _proxy = (True,)
        _nn_proxy = (False,)
        varsymbol = (None,)
        rebuild_on_cycle = (True,)
        &#34;&#34;&#34;
        # The indices variable defines how many &#34;level set&#34; maps we create as components in the proxy variable

        import sympy

        self._MaskArray = sympy.Matrix.zeros(1, self.indices)
        self._meshLevelSetVars = [None] * self.indices

        for i in range(indices):
            self._meshLevelSetVars[i] = uw.discretisation.MeshVariable(
                name + R&#34;^{[&#34; + str(i) + R&#34;]}&#34;,
                self.swarm.mesh,
                num_components=1,
                degree=proxy_degree,
                continuous=proxy_continuous,
            )
            self._MaskArray[0, i] = self._meshLevelSetVars[i].sym[0, 0]

        return

    # This is the sympy vector interface - it&#39;s meaningless if these are not spatial arrays
    @property
    def sym(self):
        return self._MaskArray

    def _update(self):
        &#34;&#34;&#34;
        This method updates the proxy mesh (vector) variable for the index variable on the current swarm locations

        Here is how it works:

            1) for each particle, create a distance-weighted average on the node data
            2) for each index in the set, we create a mask mesh variable by mapping 1.0 wherever the
               index matches and 0.0 where it does not.

        NOTE: If no material is identified with a given nodal value, the default is to material zero

        ## ToDo: This should be revisited to match the updated master copy of _update

        &#34;&#34;&#34;

        kd = uw.kdtree.KDTree(self._meshLevelSetVars[0].coords)
        kd.build_index()

        for ii in range(self.indices):
            meshVar = self._meshLevelSetVars[ii]

            # 1 - Average particles to nodes with distance weighted average
            with self.swarm.mesh.access(meshVar), self.swarm.access():
                n, d, b = kd.find_closest_point(self.swarm.data)

                node_values = np.zeros((meshVar.data.shape[0],))
                w = np.zeros((meshVar.data.shape[0],))

                for i in range(self.data.shape[0]):
                    if b[i]:
                        node_values[n[i]] += np.isclose(self.data[i], ii) / (
                            1.0e-16 + d[i]
                        )
                        w[n[i]] += 1.0 / (1.0e-16 + d[i])

                node_values[np.where(w &gt; 0.0)[0]] /= w[np.where(w &gt; 0.0)[0]]

            # 2 - set NN vals on mesh var where w == 0.0

            with self.swarm.mesh.access(meshVar), self.swarm.access():
                meshVar.data[...] = node_values[...].reshape(-1, 1)

                # Need to document this assumption, if there is no material found,
                # assume the default material (0). An alternative would be to impose
                # a near-neighbour hunt for a valid material and set that one.

                if ii == 0:
                    meshVar.data[np.where(w == 0.0)] = 1.0
                else:
                    meshVar.data[np.where(w == 0.0)] = 0.0

        return


# @typechecked
class Swarm(_api_tools.Stateful):

    instances = 0

    @timing.routine_timer_decorator
    def __init__(self, mesh, recycle_rate=0):

        Swarm.instances += 1

        self._mesh = mesh
        self.dim = mesh.dim
        self.cdim = mesh.cdim
        self.dm = PETSc.DMSwarm().create()
        self.dm.setDimension(self.dim)
        self.dm.setType(SwarmType.DMSWARM_PIC.value)
        self.dm.setCellDM(mesh.dm)
        self._data = None

        # Is the swarm a streak-swarm ?
        self.recycle_rate = recycle_rate
        self.cycle = 0

        # dictionary for variables

        # import weakref (not helpful as garbage collection does not remove the fields from the DM)
        # self._vars = weakref.WeakValueDictionary()
        self._vars = {}

        # add variable to handle particle coords - predefined by DMSwarm, expose to UW
        self._coord_var = SwarmVariable(
            &#34;DMSwarmPIC_coor&#34;,
            self,
            self.cdim,
            dtype=float,
            _register=False,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # add variable to handle particle cell id - predefined by DMSwarm, expose to UW
        self._cellid_var = SwarmVariable(
            &#34;DMSwarm_cellid&#34;,
            self,
            1,
            dtype=int,
            _register=False,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # add variable to hold swarm coordinates during position updates
        self._X0 = uw.swarm.SwarmVariable(
            &#34;DMSwarm_X0&#34;,
            self,
            self.cdim,
            dtype=float,
            _register=True,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # This is for swarm streak management:
        # add variable to hold swarm origins

        if self.recycle_rate &gt; 1:
            # self._Xorig = uw.swarm.SwarmVariable(
            #     &#34;DMSwarm_Xorig&#34;,
            #     self,
            #     self.cdim,
            #     dtype=float,
            #     _register=True,
            #     _proxy=False,
            #     rebuild_on_cycle=False,
            # )

            self._remeshed = uw.swarm.SwarmVariable(
                &#34;DMSwarm_remeshed&#34;,
                self,
                1,
                dtype=int,
                _register=True,
                _proxy=False,
                rebuild_on_cycle=False,
            )

        self._X0_uninitialised = True
        # self._Xorig_uninitialised = True
        self._index = None
        self._nnmapdict = {}

        super().__init__()

    @property
    def mesh(self):
        return self._mesh

    # The setter needs updating to account for re-distribution of the DM
    # in the general case - see adaptivity.mesh2mesh_swarm()

    # @mesh.setter
    # def mesh(self, new_mesh):
    #     self._mesh = new_mesh
    #     self.dm.setCellDM(new_mesh.dm)

    #     # k-d tree indexing is no longer valid
    #     self._index = None
    #     self._nnmapdict = {}

    #     cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    #     cellid[:] = 0  # new_mesh.get_closest_cells(coords).reshape(-1)
    #     self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
    #     self.dm.migrate(remove_sent_points=True)

    #     # Also need to re-proxy the swarm variables on the new mesh !!
    #     for v in self.vars:
    #         var = self.vars[v]
    #         var._create_proxy_variable()
    #         var._update()

    #     return

    @property
    def data(self):
        return self.particle_coordinates.data

    @property
    def particle_coordinates(self):
        return self._coord_var

    @property
    def particle_cellid(self):
        return self._cellid_var

    # @timing.routine_timer_decorator
    # def populate(
    #     self,
    #     fill_param: Optional[int] = 3,
    #     layout: Optional[SwarmPICLayout] = None,
    # ):
    #     (
    #         &#34;&#34;&#34;
    #     Populate the swarm with particles throughout the domain.

    #     &#34;&#34;&#34;
    #         + SwarmPICLayout.__doc__
    #         + &#34;&#34;&#34;

    #     When using SwarmPICLayout.REGULAR,     `fill_param` defines the number of points in each spatial direction.
    #     When using SwarmPICLayout.GAUSS,       `fill_param` defines the number of quadrature points in each spatial direction.
    #     When using SwarmPICLayout.SUBDIVISION, `fill_param` defines the number times the reference cell is sub-divided.

    #     Parameters
    #     ----------
    #     fill_param:
    #         Parameter determining the particle count per cell for the given layout.
    #     layout:
    #         Type of layout to use. Defaults to `SwarmPICLayout.REGULAR` for mesh objects with simplex
    #         type cells, and `SwarmPICLayout.GAUSS` otherwise.

    #     &#34;&#34;&#34;
    #     )

    #     self.fill_param = fill_param

    #     &#34;&#34;&#34;
    #     Currently (2021.11.15) supported by PETSc release 3.16.x

    #     When using a DMPLEX the following case are supported:
    #           (i) DMSWARMPIC_LAYOUT_REGULAR: 2D (triangle),
    #          (ii) DMSWARMPIC_LAYOUT_GAUSS: 2D and 3D provided the cell is a tri/tet or a quad/hex,
    #         (iii) DMSWARMPIC_LAYOUT_SUBDIVISION: 2D and 3D for quad/hex and 2D tri.

    #     So this means, simplex mesh in 3D only supports GAUSS - This is based
    #     on the tensor product locations so it is not even in the cells.

    #     &#34;&#34;&#34;

    @timing.routine_timer_decorator
    def populate(
        self,
        fill_param: Optional[int] = 1,
    ):
        (
            &#34;&#34;&#34;
        Populate the swarm with particles throughout the domain.

        Parameters
        ----------
        fill_param:
            Parameter determining the particle count per cell (per dimension)
            for the given layout, using the mesh degree.

        cell_search:
            Use k-d tree to locate nearest cells (fails if this swarm is used to build a k-d tree)

        &#34;&#34;&#34;
        )

        self.fill_param = fill_param

        newp_coords0 = self.mesh._get_coords_for_basis(fill_param, continuous=False)
        newp_cells0 = self.mesh.get_closest_local_cells(newp_coords0)

        valid = newp_cells0 != -1
        newp_coords = newp_coords0[valid]
        newp_cells = newp_cells0[valid]

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(newp_coords.shape[0] + 1)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[...] = newp_coords[...]
        cellid[:] = newp_cells[:]

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        ## Now make a series of copies to allow the swarm cycling to
        ## work correctly (if required)

        # cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        # lost = np.where(cellid == -1)
        # print(f&#34;{uw.mpi.rank} - lost particles: {lost[0].shape} out of {cellid.shape}&#34;, flush=True)
        # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        if self.recycle_rate &gt; 1:
            with self.access():
                # Actually, this is a mesh-local quantity, so let&#39;s just
                # store it on the mesh in an ad_hoc fashion for now

                self.mesh.particle_X_orig = self.particle_coordinates.data.copy()
                self.mesh.particle_CellID_orig = self._cellid_var.data.copy()

            with self.access():
                swarm_orig_size = self.particle_coordinates.data.shape[0]
                all_local_coords = np.vstack(
                    (self.particle_coordinates.data,) * (self.recycle_rate)
                )
                all_local_cells = np.vstack(
                    (self._cellid_var.data,) * (self.recycle_rate)
                )

                swarm_new_size = all_local_coords.data.shape[0]

            self.dm.addNPoints(swarm_new_size - swarm_orig_size)

            cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
            coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

            coords[...] = (
                all_local_coords[...]
                + (0.33 / (1 + fill_param))
                * (np.random.random(size=all_local_coords.shape) - 0.5)
                * self.mesh._search_lengths[all_local_cells]  # typical cell size
            )
            cellid[:] = all_local_cells[:, 0]

            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
            self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

            ## Now set the cycle values

            with self.access(self._remeshed):
                for i in range(0, self.recycle_rate):
                    offset = swarm_orig_size * i
                    self._remeshed.data[offset::, 0] = i

        # Validate (eliminate if required)

        # cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        # lost = np.where(cellid == -1)
        # print(f&#34;{uw.mpi.rank} - lost particles: {lost[0].shape} out of {cellid.shape}&#34;, flush=True)
        # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        return

    ## This is actually an initial population routine.
    ## We can&#39;t use this to add particles / manage variables (LM)
    @timing.routine_timer_decorator
    def add_particles_with_coordinates(self, coordinatesArray):
        &#34;&#34;&#34;
        This method adds particles to the swarm using particle coordinates provided
        using a numpy array.
        Note that particles with coordinates NOT local to the current processor will
        be reject/ignored. Either include an array with all coordinates to all processors
        or an array with the local coordinates.
        Parameters
        ----------
        coordinatesArray : numpy.ndarray
            The numpy array containing the coordinate of the new particles. Array is
            expected to take shape n*dim, where n is the number of new particles, and
            dim is the dimensionality of the swarm&#39;s supporting mesh.
        &#34;&#34;&#34;

        if not isinstance(coordinatesArray, np.ndarray):
            raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
        if not len(coordinatesArray.shape) == 2:
            raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
        if not coordinatesArray.shape[1] == self.mesh.dim:
            #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
            raise ValueError(
                &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                              number of particles to add, and &#39;dim&#39; is the dimensionality of
                              the supporting mesh ({}).&#34;&#34;&#34;.format(
                    self.mesh.dim
                )
            )

        cells = self.mesh.get_closest_local_cells(coordinatesArray)

        valid_coordinates = coordinatesArray[cells != -1]
        valid_cells = cells[cells != -1]

        npoints = len(valid_coordinates)
        swarm_size = self.dm.getLocalSize()

        # -1 means no particles have been added yet
        if swarm_size == -1:
            swarm_size = 0
            npoints = npoints + 1

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(npoints=npoints)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[swarm_size::, :] = valid_coordinates[:, :]
        cellid[swarm_size::] = valid_cells[:]

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        # Here we update the swarm cycle values as required

        if self.recycle_rate &gt; 1:
            with self.access(self._remeshed):
                # self._Xorig.data[...] = coordinatesArray
                self._remeshed.data[...] = 0

        self.dm.migrate(remove_sent_points=True)

        return

    @timing.routine_timer_decorator
    def save(
        self,
        filename: int,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential=False,
    ):
        &#34;&#34;&#34;

        Save the swarm coordinates to a h5 file.

        Parameters
        ----------
        filename :
            The filename of the swarm checkpoint file to save to disk.
        compression :
            Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
        compressionType :
            Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.



        &#34;&#34;&#34;
        if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
            warnings.warn(
                &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
                stacklevel=2,
            )
        if filename.endswith(&#34;.h5&#34;) == False:
            raise RuntimeError(&#34;The filename must end with .h5&#34;)
        if compression == True and comm.rank == 0:
            warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)

        if h5py.h5.get_config().mpi == True and not force_sequential:

            # It seems to be a bad idea to mix mpi barriers with the access
            # context manager so the copy-free version of this seems to hang
            # when there are many active cores. This is probably why the parallel
            # h5py write hangs

            with self.access():
                data_copy = self.data[:].copy()

            with h5py.File(
                f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=MPI.COMM_WORLD
            ) as h5f:
                if compression == True:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy[:],
                        compression=compressionType,
                    )
                else:
                    h5f.create_dataset(&#34;coordinates&#34;, data=data_copy[:])
        else:

            # It seems to be a bad idea to mix mpi barriers with the access
            # context manager so the copy-free version of this seems to hang
            # when there are many active cores

            with self.access():
                data_copy = self.data[:].copy()

            if comm.rank == 0:
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                    if compression == True:
                        h5f.create_dataset(
                            &#34;coordinates&#34;,
                            data=data_copy,
                            chunks=True,
                            maxshape=(None, data_copy.shape[1]),
                            compression=compressionType,
                        )
                    else:
                        h5f.create_dataset(
                            &#34;coordinates&#34;,
                            data=data_copy,
                            chunks=True,
                            maxshape=(None, data_copy.shape[1]),
                        )

            comm.barrier()
            for i in range(1, comm.size):
                if comm.rank == i:
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                        h5f[&#34;coordinates&#34;].resize(
                            (h5f[&#34;coordinates&#34;].shape[0] + data_copy.shape[0]),
                            axis=0,
                        )
                        # passive swarm, zero local particles is not unusual
                        if data_copy.shape[0] &gt; 0:
                            h5f[&#34;coordinates&#34;][-data_copy.shape[0] :] = data_copy[:]
                comm.barrier()
            comm.barrier()

        return

    @timing.routine_timer_decorator
    def load(
        self,
        filename: str,
    ):
        ### open up file with coords on all procs
        with h5py.File(f&#34;{filename}&#34;, &#34;r&#34;) as h5f:
            coordinates = h5f[&#34;coordinates&#34;][:]

        #### utilises the UW function for adding a swarm by an array
        self.add_particles_with_coordinates(coordinates)

        return

    @timing.routine_timer_decorator
    def add_variable(
        self,
        name,
        size=1,
        dtype=float,
        proxy_degree=2,
        _nn_proxy=False,
    ):

        return SwarmVariable(
            name,
            self,
            size,
            dtype=dtype,
            proxy_degree=proxy_degree,
            _nn_proxy=_nn_proxy,
        )

    @timing.routine_timer_decorator
    def petsc_save_checkpoint(
        self,
        swarmName: str,
        index: int,
        outputPath: Optional[str] = &#34;&#34;,
    ):

        &#34;&#34;&#34;

        Use PETSc to save the swarm and attached data to a .pbin and xdmf file.

        Parameters
        ----------
        swarmName :
            Name of the swarm to save.
        index :
            An index which might correspond to the timestep or output number (for example).
        outputPath :
            Path to save the data. If left empty it will save the data in the current working directory.
        &#34;&#34;&#34;

        x_swarm_fname = f&#34;{outputPath}{swarmName}_{index:04d}.xmf&#34;
        self.dm.viewXDMF(x_swarm_fname)

    @timing.routine_timer_decorator
    def save_checkpoint(
        self,
        swarmName: str,
        swarmVars: list,
        index: int,
        outputPath: Optional[str] = &#34;&#34;,
        time: Optional[int] = None,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential: Optional[bool] = False,
    ):

        &#34;&#34;&#34;

        Save data to h5 and a corresponding xdmf for visualisation using h5py.

        Parameters
        ----------
        swarmName :
            Name of the swarm to save.
        swarmVars :
            List of swarm objects to save.
        index :
            An index which might correspond to the timestep or output number (for example).
        outputPath :
            Path to save the data. If left empty it will save the data in the current working directory.
        time :
            Attach the time to the generated xdmf.
        compression :
            Whether to compress the h5 files [bool].
        compressionType :
            The type of compression to use. &#39;gzip&#39; and &#39;lzf&#39; are the supported types, with &#39;gzip&#39; as the default.
        &#34;&#34;&#34;

        if swarmVars != None and not isinstance(swarmVars, list):
            raise RuntimeError(&#34;`swarmVars` does not appear to be a list.&#34;)

        else:
            ### save the swarm particle location
            self.save(
                filename=f&#34;{outputPath}{swarmName}-{index:04d}.h5&#34;,
                compression=compression,
                compressionType=compressionType,
                force_sequential=force_sequential,
            )

        #### Generate a h5 file for each field
        if swarmVars != None:
            for field in swarmVars:
                field.save(
                    filename=f&#34;{outputPath}{field.name}-{index:04d}.h5&#34;,
                    compression=compression,
                    compressionType=compressionType,
                    force_sequential=force_sequential,
                )

        if uw.mpi.rank == 0:
            ### only need to combine the h5 files to a single xdmf on one proc
            with open(f&#34;{outputPath}{swarmName}-{index:04d}.xmf&#34;, &#34;w&#34;) as xdmf:
                # Write the XDMF header
                xdmf.write(&#39;&lt;?xml version=&#34;1.0&#34; ?&gt;\n&#39;)
                xdmf.write(
                    &#39;&lt;Xdmf xmlns:xi=&#34;http://www.w3.org/2001/XInclude&#34; Version=&#34;2.0&#34;&gt;\n&#39;
                )
                xdmf.write(&#34;&lt;Domain&gt;\n&#34;)
                xdmf.write(
                    f&#39;&lt;Grid Name=&#34;{swarmName}-{index:04d}&#34; GridType=&#34;Uniform&#34;&gt;\n&#39;
                )

                if time != None:
                    xdmf.write(f&#39;       &lt;Time Value=&#34;{time}&#34; /&gt;\n&#39;)

                # Write the grid element for the HDF5 dataset
                with h5py.File(f&#34;{outputPath}{swarmName}-{index:04}.h5&#34;, &#34;r&#34;) as h5f:
                    xdmf.write(
                        f&#39;      &lt;Topology Type=&#34;POLYVERTEX&#34; NodesPerElement=&#34;{h5f[&#34;coordinates&#34;].shape[0]}&#34;&gt; &lt;/Topology&gt;\n&#39;
                    )
                    if h5f[&#34;coordinates&#34;].shape[1] == 2:
                        xdmf.write(&#39;            &lt;Geometry Type=&#34;XY&#34;&gt;\n&#39;)
                    elif h5f[&#34;coordinates&#34;].shape[1] == 3:
                        xdmf.write(&#39;            &lt;Geometry Type=&#34;XYZ&#34;&gt;\n&#39;)
                    xdmf.write(
                        f&#39;                      &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;coordinates&#34;].shape[0]} {h5f[&#34;coordinates&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/coordinates&lt;/DataItem&gt;\n&#39;
                    )
                    xdmf.write(&#34;                &lt;/Geometry&gt;\n&#34;)

                # Write the attribute element for the field
                if swarmVars != None:
                    for field in swarmVars:
                        with h5py.File(
                            f&#34;{outputPath}{field.name}-{index:04d}.h5&#34;, &#34;r&#34;
                        ) as h5f:
                            if h5f[&#34;data&#34;].dtype == np.int32:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Int&#34; Precision=&#34;4&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            elif h5f[&#34;data&#34;].shape[1] == 1:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            elif h5f[&#34;data&#34;].shape[1] == 2 or h5f[&#34;data&#34;].shape[1] == 3:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Vector&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            else:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Tensor&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )

                            xdmf.write(&#34;        &lt;/Attribute&gt;\n&#34;)
                else:
                    pass

                # Write the XDMF footer
                xdmf.write(&#34;&lt;/Grid&gt;\n&#34;)
                xdmf.write(&#34;&lt;/Domain&gt;\n&#34;)
                xdmf.write(&#34;&lt;/Xdmf&gt;\n&#34;)

    @property
    def vars(self):
        return self._vars

    def access(self, *writeable_vars: SwarmVariable):
        &#34;&#34;&#34;
        This context manager makes the underlying swarm variables data available to
        the user. The data should be accessed via the variables `data` handle.

        As default, all data is read-only. To enable writeable data, the user should
        specify which variable they wish to modify.

        At the conclusion of the users context managed block, numerous further operations
        will be automatically executed. This includes swarm parallel migration routines
        where the swarm&#39;s `particle_coordinates` variable has been modified. The swarm
        variable proxy mesh variables will also be updated for modifed swarm variables.

        Parameters
        ----------
        writeable_vars
            The variables for which data write access is required.

        Example
        -------

        &gt;&gt;&gt; import underworld3 as uw
        &gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
        &gt;&gt;&gt; with someMesh.deform_mesh():
        ...     someMesh.data[0] = [0.1,0.1]
        &gt;&gt;&gt; someMesh.data[0]
        array([ 0.1,  0.1])
        &#34;&#34;&#34;
        import time

        uw.timing._incrementDepth()
        stime = time.time()

        deaccess_list = []
        for var in self._vars.values():
            # if already accessed within higher level context manager, continue.
            if var._is_accessed == True:
                continue
            # set flag so variable status can be known elsewhere
            var._is_accessed = True
            # add to de-access list to rewind this later
            deaccess_list.append(var)
            # grab numpy object, setting read only if necessary
            var._data = self.dm.getField(var.clean_name).reshape(
                (-1, var.num_components)
            )
            if var not in writeable_vars:
                var._old_data_flag = var._data.flags.writeable
                var._data.flags.writeable = False
            else:
                # increment variable state
                var._increment()
        # if particles moving, update swarm state
        if self.particle_coordinates in writeable_vars:
            self._increment()

        # Create a class which specifies the required context
        # manager hooks (`__enter__`, `__exit__`).
        class exit_manager:
            def __init__(self, swarm):
                self.em_swarm = swarm

            def __enter__(self):
                pass

            def __exit__(self, *args):
                for var in self.em_swarm.vars.values():
                    # only de-access variables we have set access for.
                    if var not in deaccess_list:
                        continue
                    # set this back, although possibly not required.
                    if var not in writeable_vars:
                        var._data.flags.writeable = var._old_data_flag
                    var._data = None
                    self.em_swarm.dm.restoreField(var.clean_name)
                    var._is_accessed = False
                # do particle migration if coords changes
                if self.em_swarm.particle_coordinates in writeable_vars:
                    # let&#39;s use the mesh index to update the particles owning cells.
                    # note that the `petsc4py` interface is more convenient here as the
                    # `SwarmVariable.data` interface is controlled by the context manager
                    # that we are currently within, and it is therefore too easy to
                    # get things wrong that way.
                    cellid = self.em_swarm.dm.getField(&#34;DMSwarm_cellid&#34;)
                    coords = self.em_swarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                        (-1, self.em_swarm.dim)
                    )
                    cellid[:] = self.em_swarm.mesh.get_closest_cells(coords).reshape(-1)
                    self.em_swarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
                    self.em_swarm.dm.restoreField(&#34;DMSwarm_cellid&#34;)
                    # now migrate.
                    self.em_swarm.dm.migrate(remove_sent_points=True)
                    # void these things too
                    self.em_swarm._index = None
                    self.em_swarm._nnmapdict = {}
                # do var updates
                for var in self.em_swarm.vars.values():
                    # if swarm migrated, update all.
                    # if var updated, update var.
                    if (self.em_swarm.particle_coordinates in writeable_vars) or (
                        var in writeable_vars
                    ):
                        var._update()

                uw.timing._decrementDepth()
                uw.timing.log_result(time.time() - stime, &#34;Swarm.access&#34;, 1)

        return exit_manager(self)

    @timing.routine_timer_decorator
    def _get_map(self, var):
        # generate tree if not avaiable
        if not self._index:
            with self.access():
                self._index = uw.kdtree.KDTree(self.data)

        # get or generate map
        meshvar_coords = var._meshVar.coords
        # we can&#39;t use numpy arrays directly as keys in python dicts, so
        # we&#39;ll use `xxhash` to generate a hash of array.
        # this shouldn&#39;t be an issue performance wise but we should test to be
        # sufficiently confident of this.
        import xxhash

        h = xxhash.xxh64()
        h.update(meshvar_coords)
        digest = h.intdigest()
        if digest not in self._nnmapdict:
            self._nnmapdict[digest] = self._index.find_closest_point(meshvar_coords)[0]
        return self._nnmapdict[digest]

    @timing.routine_timer_decorator
    def advection(
        self,
        V_fn,
        delta_t,
        order=2,
        corrector=False,
        restore_points_to_domain_func=None,
    ):

        # X0 holds the particle location at the start of advection
        # This is needed because the particles may be migrated off-proc
        # during timestepping.

        X0 = self._X0

        # Use current velocity to estimate where the particles would have
        # landed in an implicit step.

        # ? how does this interact with the particle restoration function ?

        V_fn_matrix = self.mesh.vector.to_matrix(V_fn)

        if corrector == True and not self._X0_uninitialised:
            with self.access(self.particle_coordinates):
                v_at_Vpts = np.zeros_like(self.data)

                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d], self.data
                    ).reshape(-1)

                corrected_position = X0.data + delta_t * v_at_Vpts
                if restore_points_to_domain_func is not None:
                    corrected_position = restore_points_to_domain_func(
                        corrected_position
                    )

                updated_current_coords = 0.5 * (corrected_position + self.data)

                # validate_coords to ensure they live within the domain (or there will be trouble)

                if restore_points_to_domain_func is not None:
                    updated_current_coords = restore_points_to_domain_func(
                        updated_current_coords
                    )

                self.data[...] = updated_current_coords[...]

        with self.access(X0):
            X0.data[...] = self.data[...]
            self._X0_uninitialised = False

        # Mid point algorithm (2nd order)
        if order == 2:
            with self.access(self.particle_coordinates):

                v_at_Vpts = np.zeros_like(self.data)

                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d], self.data
                    ).reshape(-1)

                mid_pt_coords = self.data[...] + 0.5 * delta_t * v_at_Vpts

                # validate_coords to ensure they live within the domain (or there will be trouble)

                if restore_points_to_domain_func is not None:
                    mid_pt_coords = restore_points_to_domain_func(mid_pt_coords)

                self.data[...] = mid_pt_coords[...]

                ## Let the swarm be updated, and then move the rest of the way

            with self.access(self.particle_coordinates):

                v_at_Vpts = np.zeros_like(self.data)

                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d], self.data
                    ).reshape(-1)

                # if (uw.mpi.rank == 0):
                #     print(&#34;Re-launch from X0&#34;, flush=True)

                new_coords = X0.data[...] + delta_t * v_at_Vpts

                # validate_coords to ensure they live within the domain (or there will be trouble)
                if restore_points_to_domain_func is not None:
                    new_coords = restore_points_to_domain_func(new_coords)

                self.data[...] = new_coords[...]

        # Previous position algorithm (cf above) - we use the previous step as the
        # launch point using the current velocity field. This gives a correction to the previous
        # landing point.

        # assumes X0 is stored from the previous step ... midpoint is needed in the first step

        # forward Euler (1st order)
        else:
            with self.access(self.particle_coordinates):
                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(V_fn[d], self.data).reshape(
                        -1
                    )

                new_coords = self.data + delta_t * v_at_Vpts

                # validate_coords to ensure they live within the domain (or there will be trouble)

                if restore_points_to_domain_func is not None:
                    new_coords = restore_points_to_domain_func(new_coords)

                self.data[...] = new_coords

        ## Cycling of the swarm is a cheap and cheerful version of population control for particles. It turns the
        ## swarm into a streak-swarm where particles are Lagrangian for a number of steps and then reset to their
        ## original location.

        if self.recycle_rate &gt; 1:
            # Restore particles which have cycle == cycle rate (use &gt;= just in case)

            # Remove remesh points and recreate a new set at the mesh-local
            # locations that we already have stored.

            with self.access(self.particle_coordinates, self._remeshed):
                remeshed = self._remeshed.data[:, 0] == 0
                # This is one way to do it ... we can do this better though
                self.data[remeshed, 0] = 1.0e100

            swarm_size = self.dm.getLocalSize()

            num_remeshed_points = self.mesh.particle_X_orig.shape[0]

            self.dm.addNPoints(num_remeshed_points)

            cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
            coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
            rmsh = self.dm.getField(&#34;DMSwarm_remeshed&#34;)

            # print(f&#34;cellid -&gt; {cellid.shape}&#34;)
            # print(f&#34;particle coords -&gt; {coords.shape}&#34;)
            # print(f&#34;remeshed points  -&gt; {num_remeshed_points}&#34;)

            perturbation = (
                (0.33 / (1 + self.fill_param))
                * (np.random.random(size=(num_remeshed_points, self.dim)) - 0.5)
                * self.mesh._radii[cellid[swarm_size::]].reshape(-1, 1)
            )

            # print(f&#34;{perturbation}&#34;)

            coords[swarm_size::] = self.mesh.particle_X_orig[:, :] + perturbation
            cellid[swarm_size::] = self.mesh.particle_CellID_orig[:, 0]
            rmsh[swarm_size::] = 0

            self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
            self.dm.restoreField(&#34;DMSwarm_remeshed&#34;)

            # when we let this go, the particles may be re-distributed to
            # other processors, and we will need to rebuild the remeshed
            # array before trying to compute / assign values to variables

            for swarmVar in self.vars.values():
                if swarmVar._rebuild_on_cycle:
                    with self.access(swarmVar):

                        if swarmVar.dtype is int:
                            nnn = 1
                        else:
                            nnn = self.mesh.dim + 1  # 3 for triangles, 4 for tets ...

                        interpolated_values = (
                            swarmVar.rbf_interpolate(self.mesh.particle_X_orig, nnn=nnn)
                            # uw.function.evaluate(swarmVar._meshVar.fn, remeshed_coords)
                        ).astype(swarmVar.dtype)

                        swarmVar.data[swarm_size::] = interpolated_values

            self.dm.migrate(remove_sent_points=True)

            with self.access(self._remeshed):
                self._remeshed.data[...] = np.mod(
                    self._remeshed.data[...] - 1, self.recycle_rate
                )

            self.cycle += 1

        return</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="underworld3.swarm.IndexSwarmVariable"><code class="flex name class">
<span>class <span class="ident">IndexSwarmVariable</span></span>
<span>(</span><span>name, swarm, indices=1, proxy_degree=1, proxy_continuous=True)</span>
</code></dt>
<dd>
<div class="desc"><p>The IndexSwarmVariable is a class for managing material point
behaviour. The material index variable is rendered into a
collection of masks each representing the extent of one material</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IndexSwarmVariable(SwarmVariable):
    &#34;&#34;&#34;
    The IndexSwarmVariable is a class for managing material point
    behaviour. The material index variable is rendered into a
    collection of masks each representing the extent of one material
    &#34;&#34;&#34;

    @timing.routine_timer_decorator
    def __init__(
        self,
        name,
        swarm,
        indices=1,
        proxy_degree=1,
        proxy_continuous=True,
    ):

        self.indices = indices

        # These are the things we require of the generic swarm variable type
        super().__init__(
            name,
            swarm,
            size=1,
            vtype=None,
            dtype=int,
            _proxy=False,
        )
        &#34;&#34;&#34;
        vtype = (None,)
        dtype = (float,)
        proxy_degree = (1,)
        proxy_continuous = (True,)
        _register = (True,)
        _proxy = (True,)
        _nn_proxy = (False,)
        varsymbol = (None,)
        rebuild_on_cycle = (True,)
        &#34;&#34;&#34;
        # The indices variable defines how many &#34;level set&#34; maps we create as components in the proxy variable

        import sympy

        self._MaskArray = sympy.Matrix.zeros(1, self.indices)
        self._meshLevelSetVars = [None] * self.indices

        for i in range(indices):
            self._meshLevelSetVars[i] = uw.discretisation.MeshVariable(
                name + R&#34;^{[&#34; + str(i) + R&#34;]}&#34;,
                self.swarm.mesh,
                num_components=1,
                degree=proxy_degree,
                continuous=proxy_continuous,
            )
            self._MaskArray[0, i] = self._meshLevelSetVars[i].sym[0, 0]

        return

    # This is the sympy vector interface - it&#39;s meaningless if these are not spatial arrays
    @property
    def sym(self):
        return self._MaskArray

    def _update(self):
        &#34;&#34;&#34;
        This method updates the proxy mesh (vector) variable for the index variable on the current swarm locations

        Here is how it works:

            1) for each particle, create a distance-weighted average on the node data
            2) for each index in the set, we create a mask mesh variable by mapping 1.0 wherever the
               index matches and 0.0 where it does not.

        NOTE: If no material is identified with a given nodal value, the default is to material zero

        ## ToDo: This should be revisited to match the updated master copy of _update

        &#34;&#34;&#34;

        kd = uw.kdtree.KDTree(self._meshLevelSetVars[0].coords)
        kd.build_index()

        for ii in range(self.indices):
            meshVar = self._meshLevelSetVars[ii]

            # 1 - Average particles to nodes with distance weighted average
            with self.swarm.mesh.access(meshVar), self.swarm.access():
                n, d, b = kd.find_closest_point(self.swarm.data)

                node_values = np.zeros((meshVar.data.shape[0],))
                w = np.zeros((meshVar.data.shape[0],))

                for i in range(self.data.shape[0]):
                    if b[i]:
                        node_values[n[i]] += np.isclose(self.data[i], ii) / (
                            1.0e-16 + d[i]
                        )
                        w[n[i]] += 1.0 / (1.0e-16 + d[i])

                node_values[np.where(w &gt; 0.0)[0]] /= w[np.where(w &gt; 0.0)[0]]

            # 2 - set NN vals on mesh var where w == 0.0

            with self.swarm.mesh.access(meshVar), self.swarm.access():
                meshVar.data[...] = node_values[...].reshape(-1, 1)

                # Need to document this assumption, if there is no material found,
                # assume the default material (0). An alternative would be to impose
                # a near-neighbour hunt for a valid material and set that one.

                if ii == 0:
                    meshVar.data[np.where(w == 0.0)] = 1.0
                else:
                    meshVar.data[np.where(w == 0.0)] = 0.0

        return</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></li>
<li>underworld3.utilities._api_tools.Stateful</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="underworld3.swarm.IndexSwarmVariable.sym"><code class="name">var <span class="ident">sym</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sym(self):
    return self._MaskArray</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></b></code>:
<ul class="hlist">
<li><code><a title="underworld3.swarm.SwarmVariable.save" href="#underworld3.swarm.SwarmVariable.save">save</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="underworld3.swarm.Swarm"><code class="flex name class">
<span>class <span class="ident">Swarm</span></span>
<span>(</span><span>mesh, recycle_rate=0)</span>
</code></dt>
<dd>
<div class="desc"><p>This is a mixin class for underworld objects that are stateful.
The state of an object is incremented whenever it is modified.
For example, heavy variables have states, and when a user modifies
it within its <code>access()</code> context manager, its state is incremented
at the conclusion of their modifications.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Swarm(_api_tools.Stateful):

    instances = 0

    @timing.routine_timer_decorator
    def __init__(self, mesh, recycle_rate=0):

        Swarm.instances += 1

        self._mesh = mesh
        self.dim = mesh.dim
        self.cdim = mesh.cdim
        self.dm = PETSc.DMSwarm().create()
        self.dm.setDimension(self.dim)
        self.dm.setType(SwarmType.DMSWARM_PIC.value)
        self.dm.setCellDM(mesh.dm)
        self._data = None

        # Is the swarm a streak-swarm ?
        self.recycle_rate = recycle_rate
        self.cycle = 0

        # dictionary for variables

        # import weakref (not helpful as garbage collection does not remove the fields from the DM)
        # self._vars = weakref.WeakValueDictionary()
        self._vars = {}

        # add variable to handle particle coords - predefined by DMSwarm, expose to UW
        self._coord_var = SwarmVariable(
            &#34;DMSwarmPIC_coor&#34;,
            self,
            self.cdim,
            dtype=float,
            _register=False,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # add variable to handle particle cell id - predefined by DMSwarm, expose to UW
        self._cellid_var = SwarmVariable(
            &#34;DMSwarm_cellid&#34;,
            self,
            1,
            dtype=int,
            _register=False,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # add variable to hold swarm coordinates during position updates
        self._X0 = uw.swarm.SwarmVariable(
            &#34;DMSwarm_X0&#34;,
            self,
            self.cdim,
            dtype=float,
            _register=True,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # This is for swarm streak management:
        # add variable to hold swarm origins

        if self.recycle_rate &gt; 1:
            # self._Xorig = uw.swarm.SwarmVariable(
            #     &#34;DMSwarm_Xorig&#34;,
            #     self,
            #     self.cdim,
            #     dtype=float,
            #     _register=True,
            #     _proxy=False,
            #     rebuild_on_cycle=False,
            # )

            self._remeshed = uw.swarm.SwarmVariable(
                &#34;DMSwarm_remeshed&#34;,
                self,
                1,
                dtype=int,
                _register=True,
                _proxy=False,
                rebuild_on_cycle=False,
            )

        self._X0_uninitialised = True
        # self._Xorig_uninitialised = True
        self._index = None
        self._nnmapdict = {}

        super().__init__()

    @property
    def mesh(self):
        return self._mesh

    # The setter needs updating to account for re-distribution of the DM
    # in the general case - see adaptivity.mesh2mesh_swarm()

    # @mesh.setter
    # def mesh(self, new_mesh):
    #     self._mesh = new_mesh
    #     self.dm.setCellDM(new_mesh.dm)

    #     # k-d tree indexing is no longer valid
    #     self._index = None
    #     self._nnmapdict = {}

    #     cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    #     cellid[:] = 0  # new_mesh.get_closest_cells(coords).reshape(-1)
    #     self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
    #     self.dm.migrate(remove_sent_points=True)

    #     # Also need to re-proxy the swarm variables on the new mesh !!
    #     for v in self.vars:
    #         var = self.vars[v]
    #         var._create_proxy_variable()
    #         var._update()

    #     return

    @property
    def data(self):
        return self.particle_coordinates.data

    @property
    def particle_coordinates(self):
        return self._coord_var

    @property
    def particle_cellid(self):
        return self._cellid_var

    # @timing.routine_timer_decorator
    # def populate(
    #     self,
    #     fill_param: Optional[int] = 3,
    #     layout: Optional[SwarmPICLayout] = None,
    # ):
    #     (
    #         &#34;&#34;&#34;
    #     Populate the swarm with particles throughout the domain.

    #     &#34;&#34;&#34;
    #         + SwarmPICLayout.__doc__
    #         + &#34;&#34;&#34;

    #     When using SwarmPICLayout.REGULAR,     `fill_param` defines the number of points in each spatial direction.
    #     When using SwarmPICLayout.GAUSS,       `fill_param` defines the number of quadrature points in each spatial direction.
    #     When using SwarmPICLayout.SUBDIVISION, `fill_param` defines the number times the reference cell is sub-divided.

    #     Parameters
    #     ----------
    #     fill_param:
    #         Parameter determining the particle count per cell for the given layout.
    #     layout:
    #         Type of layout to use. Defaults to `SwarmPICLayout.REGULAR` for mesh objects with simplex
    #         type cells, and `SwarmPICLayout.GAUSS` otherwise.

    #     &#34;&#34;&#34;
    #     )

    #     self.fill_param = fill_param

    #     &#34;&#34;&#34;
    #     Currently (2021.11.15) supported by PETSc release 3.16.x

    #     When using a DMPLEX the following case are supported:
    #           (i) DMSWARMPIC_LAYOUT_REGULAR: 2D (triangle),
    #          (ii) DMSWARMPIC_LAYOUT_GAUSS: 2D and 3D provided the cell is a tri/tet or a quad/hex,
    #         (iii) DMSWARMPIC_LAYOUT_SUBDIVISION: 2D and 3D for quad/hex and 2D tri.

    #     So this means, simplex mesh in 3D only supports GAUSS - This is based
    #     on the tensor product locations so it is not even in the cells.

    #     &#34;&#34;&#34;

    @timing.routine_timer_decorator
    def populate(
        self,
        fill_param: Optional[int] = 1,
    ):
        (
            &#34;&#34;&#34;
        Populate the swarm with particles throughout the domain.

        Parameters
        ----------
        fill_param:
            Parameter determining the particle count per cell (per dimension)
            for the given layout, using the mesh degree.

        cell_search:
            Use k-d tree to locate nearest cells (fails if this swarm is used to build a k-d tree)

        &#34;&#34;&#34;
        )

        self.fill_param = fill_param

        newp_coords0 = self.mesh._get_coords_for_basis(fill_param, continuous=False)
        newp_cells0 = self.mesh.get_closest_local_cells(newp_coords0)

        valid = newp_cells0 != -1
        newp_coords = newp_coords0[valid]
        newp_cells = newp_cells0[valid]

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(newp_coords.shape[0] + 1)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[...] = newp_coords[...]
        cellid[:] = newp_cells[:]

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        ## Now make a series of copies to allow the swarm cycling to
        ## work correctly (if required)

        # cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        # lost = np.where(cellid == -1)
        # print(f&#34;{uw.mpi.rank} - lost particles: {lost[0].shape} out of {cellid.shape}&#34;, flush=True)
        # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        if self.recycle_rate &gt; 1:
            with self.access():
                # Actually, this is a mesh-local quantity, so let&#39;s just
                # store it on the mesh in an ad_hoc fashion for now

                self.mesh.particle_X_orig = self.particle_coordinates.data.copy()
                self.mesh.particle_CellID_orig = self._cellid_var.data.copy()

            with self.access():
                swarm_orig_size = self.particle_coordinates.data.shape[0]
                all_local_coords = np.vstack(
                    (self.particle_coordinates.data,) * (self.recycle_rate)
                )
                all_local_cells = np.vstack(
                    (self._cellid_var.data,) * (self.recycle_rate)
                )

                swarm_new_size = all_local_coords.data.shape[0]

            self.dm.addNPoints(swarm_new_size - swarm_orig_size)

            cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
            coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

            coords[...] = (
                all_local_coords[...]
                + (0.33 / (1 + fill_param))
                * (np.random.random(size=all_local_coords.shape) - 0.5)
                * self.mesh._search_lengths[all_local_cells]  # typical cell size
            )
            cellid[:] = all_local_cells[:, 0]

            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
            self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

            ## Now set the cycle values

            with self.access(self._remeshed):
                for i in range(0, self.recycle_rate):
                    offset = swarm_orig_size * i
                    self._remeshed.data[offset::, 0] = i

        # Validate (eliminate if required)

        # cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        # lost = np.where(cellid == -1)
        # print(f&#34;{uw.mpi.rank} - lost particles: {lost[0].shape} out of {cellid.shape}&#34;, flush=True)
        # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        return

    ## This is actually an initial population routine.
    ## We can&#39;t use this to add particles / manage variables (LM)
    @timing.routine_timer_decorator
    def add_particles_with_coordinates(self, coordinatesArray):
        &#34;&#34;&#34;
        This method adds particles to the swarm using particle coordinates provided
        using a numpy array.
        Note that particles with coordinates NOT local to the current processor will
        be reject/ignored. Either include an array with all coordinates to all processors
        or an array with the local coordinates.
        Parameters
        ----------
        coordinatesArray : numpy.ndarray
            The numpy array containing the coordinate of the new particles. Array is
            expected to take shape n*dim, where n is the number of new particles, and
            dim is the dimensionality of the swarm&#39;s supporting mesh.
        &#34;&#34;&#34;

        if not isinstance(coordinatesArray, np.ndarray):
            raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
        if not len(coordinatesArray.shape) == 2:
            raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
        if not coordinatesArray.shape[1] == self.mesh.dim:
            #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
            raise ValueError(
                &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                              number of particles to add, and &#39;dim&#39; is the dimensionality of
                              the supporting mesh ({}).&#34;&#34;&#34;.format(
                    self.mesh.dim
                )
            )

        cells = self.mesh.get_closest_local_cells(coordinatesArray)

        valid_coordinates = coordinatesArray[cells != -1]
        valid_cells = cells[cells != -1]

        npoints = len(valid_coordinates)
        swarm_size = self.dm.getLocalSize()

        # -1 means no particles have been added yet
        if swarm_size == -1:
            swarm_size = 0
            npoints = npoints + 1

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(npoints=npoints)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[swarm_size::, :] = valid_coordinates[:, :]
        cellid[swarm_size::] = valid_cells[:]

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        # Here we update the swarm cycle values as required

        if self.recycle_rate &gt; 1:
            with self.access(self._remeshed):
                # self._Xorig.data[...] = coordinatesArray
                self._remeshed.data[...] = 0

        self.dm.migrate(remove_sent_points=True)

        return

    @timing.routine_timer_decorator
    def save(
        self,
        filename: int,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential=False,
    ):
        &#34;&#34;&#34;

        Save the swarm coordinates to a h5 file.

        Parameters
        ----------
        filename :
            The filename of the swarm checkpoint file to save to disk.
        compression :
            Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
        compressionType :
            Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.



        &#34;&#34;&#34;
        if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
            warnings.warn(
                &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
                stacklevel=2,
            )
        if filename.endswith(&#34;.h5&#34;) == False:
            raise RuntimeError(&#34;The filename must end with .h5&#34;)
        if compression == True and comm.rank == 0:
            warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)

        if h5py.h5.get_config().mpi == True and not force_sequential:

            # It seems to be a bad idea to mix mpi barriers with the access
            # context manager so the copy-free version of this seems to hang
            # when there are many active cores. This is probably why the parallel
            # h5py write hangs

            with self.access():
                data_copy = self.data[:].copy()

            with h5py.File(
                f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=MPI.COMM_WORLD
            ) as h5f:
                if compression == True:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy[:],
                        compression=compressionType,
                    )
                else:
                    h5f.create_dataset(&#34;coordinates&#34;, data=data_copy[:])
        else:

            # It seems to be a bad idea to mix mpi barriers with the access
            # context manager so the copy-free version of this seems to hang
            # when there are many active cores

            with self.access():
                data_copy = self.data[:].copy()

            if comm.rank == 0:
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                    if compression == True:
                        h5f.create_dataset(
                            &#34;coordinates&#34;,
                            data=data_copy,
                            chunks=True,
                            maxshape=(None, data_copy.shape[1]),
                            compression=compressionType,
                        )
                    else:
                        h5f.create_dataset(
                            &#34;coordinates&#34;,
                            data=data_copy,
                            chunks=True,
                            maxshape=(None, data_copy.shape[1]),
                        )

            comm.barrier()
            for i in range(1, comm.size):
                if comm.rank == i:
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                        h5f[&#34;coordinates&#34;].resize(
                            (h5f[&#34;coordinates&#34;].shape[0] + data_copy.shape[0]),
                            axis=0,
                        )
                        # passive swarm, zero local particles is not unusual
                        if data_copy.shape[0] &gt; 0:
                            h5f[&#34;coordinates&#34;][-data_copy.shape[0] :] = data_copy[:]
                comm.barrier()
            comm.barrier()

        return

    @timing.routine_timer_decorator
    def load(
        self,
        filename: str,
    ):
        ### open up file with coords on all procs
        with h5py.File(f&#34;{filename}&#34;, &#34;r&#34;) as h5f:
            coordinates = h5f[&#34;coordinates&#34;][:]

        #### utilises the UW function for adding a swarm by an array
        self.add_particles_with_coordinates(coordinates)

        return

    @timing.routine_timer_decorator
    def add_variable(
        self,
        name,
        size=1,
        dtype=float,
        proxy_degree=2,
        _nn_proxy=False,
    ):

        return SwarmVariable(
            name,
            self,
            size,
            dtype=dtype,
            proxy_degree=proxy_degree,
            _nn_proxy=_nn_proxy,
        )

    @timing.routine_timer_decorator
    def petsc_save_checkpoint(
        self,
        swarmName: str,
        index: int,
        outputPath: Optional[str] = &#34;&#34;,
    ):

        &#34;&#34;&#34;

        Use PETSc to save the swarm and attached data to a .pbin and xdmf file.

        Parameters
        ----------
        swarmName :
            Name of the swarm to save.
        index :
            An index which might correspond to the timestep or output number (for example).
        outputPath :
            Path to save the data. If left empty it will save the data in the current working directory.
        &#34;&#34;&#34;

        x_swarm_fname = f&#34;{outputPath}{swarmName}_{index:04d}.xmf&#34;
        self.dm.viewXDMF(x_swarm_fname)

    @timing.routine_timer_decorator
    def save_checkpoint(
        self,
        swarmName: str,
        swarmVars: list,
        index: int,
        outputPath: Optional[str] = &#34;&#34;,
        time: Optional[int] = None,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential: Optional[bool] = False,
    ):

        &#34;&#34;&#34;

        Save data to h5 and a corresponding xdmf for visualisation using h5py.

        Parameters
        ----------
        swarmName :
            Name of the swarm to save.
        swarmVars :
            List of swarm objects to save.
        index :
            An index which might correspond to the timestep or output number (for example).
        outputPath :
            Path to save the data. If left empty it will save the data in the current working directory.
        time :
            Attach the time to the generated xdmf.
        compression :
            Whether to compress the h5 files [bool].
        compressionType :
            The type of compression to use. &#39;gzip&#39; and &#39;lzf&#39; are the supported types, with &#39;gzip&#39; as the default.
        &#34;&#34;&#34;

        if swarmVars != None and not isinstance(swarmVars, list):
            raise RuntimeError(&#34;`swarmVars` does not appear to be a list.&#34;)

        else:
            ### save the swarm particle location
            self.save(
                filename=f&#34;{outputPath}{swarmName}-{index:04d}.h5&#34;,
                compression=compression,
                compressionType=compressionType,
                force_sequential=force_sequential,
            )

        #### Generate a h5 file for each field
        if swarmVars != None:
            for field in swarmVars:
                field.save(
                    filename=f&#34;{outputPath}{field.name}-{index:04d}.h5&#34;,
                    compression=compression,
                    compressionType=compressionType,
                    force_sequential=force_sequential,
                )

        if uw.mpi.rank == 0:
            ### only need to combine the h5 files to a single xdmf on one proc
            with open(f&#34;{outputPath}{swarmName}-{index:04d}.xmf&#34;, &#34;w&#34;) as xdmf:
                # Write the XDMF header
                xdmf.write(&#39;&lt;?xml version=&#34;1.0&#34; ?&gt;\n&#39;)
                xdmf.write(
                    &#39;&lt;Xdmf xmlns:xi=&#34;http://www.w3.org/2001/XInclude&#34; Version=&#34;2.0&#34;&gt;\n&#39;
                )
                xdmf.write(&#34;&lt;Domain&gt;\n&#34;)
                xdmf.write(
                    f&#39;&lt;Grid Name=&#34;{swarmName}-{index:04d}&#34; GridType=&#34;Uniform&#34;&gt;\n&#39;
                )

                if time != None:
                    xdmf.write(f&#39;       &lt;Time Value=&#34;{time}&#34; /&gt;\n&#39;)

                # Write the grid element for the HDF5 dataset
                with h5py.File(f&#34;{outputPath}{swarmName}-{index:04}.h5&#34;, &#34;r&#34;) as h5f:
                    xdmf.write(
                        f&#39;      &lt;Topology Type=&#34;POLYVERTEX&#34; NodesPerElement=&#34;{h5f[&#34;coordinates&#34;].shape[0]}&#34;&gt; &lt;/Topology&gt;\n&#39;
                    )
                    if h5f[&#34;coordinates&#34;].shape[1] == 2:
                        xdmf.write(&#39;            &lt;Geometry Type=&#34;XY&#34;&gt;\n&#39;)
                    elif h5f[&#34;coordinates&#34;].shape[1] == 3:
                        xdmf.write(&#39;            &lt;Geometry Type=&#34;XYZ&#34;&gt;\n&#39;)
                    xdmf.write(
                        f&#39;                      &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;coordinates&#34;].shape[0]} {h5f[&#34;coordinates&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/coordinates&lt;/DataItem&gt;\n&#39;
                    )
                    xdmf.write(&#34;                &lt;/Geometry&gt;\n&#34;)

                # Write the attribute element for the field
                if swarmVars != None:
                    for field in swarmVars:
                        with h5py.File(
                            f&#34;{outputPath}{field.name}-{index:04d}.h5&#34;, &#34;r&#34;
                        ) as h5f:
                            if h5f[&#34;data&#34;].dtype == np.int32:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Int&#34; Precision=&#34;4&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            elif h5f[&#34;data&#34;].shape[1] == 1:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            elif h5f[&#34;data&#34;].shape[1] == 2 or h5f[&#34;data&#34;].shape[1] == 3:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Vector&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            else:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Tensor&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )

                            xdmf.write(&#34;        &lt;/Attribute&gt;\n&#34;)
                else:
                    pass

                # Write the XDMF footer
                xdmf.write(&#34;&lt;/Grid&gt;\n&#34;)
                xdmf.write(&#34;&lt;/Domain&gt;\n&#34;)
                xdmf.write(&#34;&lt;/Xdmf&gt;\n&#34;)

    @property
    def vars(self):
        return self._vars

    def access(self, *writeable_vars: SwarmVariable):
        &#34;&#34;&#34;
        This context manager makes the underlying swarm variables data available to
        the user. The data should be accessed via the variables `data` handle.

        As default, all data is read-only. To enable writeable data, the user should
        specify which variable they wish to modify.

        At the conclusion of the users context managed block, numerous further operations
        will be automatically executed. This includes swarm parallel migration routines
        where the swarm&#39;s `particle_coordinates` variable has been modified. The swarm
        variable proxy mesh variables will also be updated for modifed swarm variables.

        Parameters
        ----------
        writeable_vars
            The variables for which data write access is required.

        Example
        -------

        &gt;&gt;&gt; import underworld3 as uw
        &gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
        &gt;&gt;&gt; with someMesh.deform_mesh():
        ...     someMesh.data[0] = [0.1,0.1]
        &gt;&gt;&gt; someMesh.data[0]
        array([ 0.1,  0.1])
        &#34;&#34;&#34;
        import time

        uw.timing._incrementDepth()
        stime = time.time()

        deaccess_list = []
        for var in self._vars.values():
            # if already accessed within higher level context manager, continue.
            if var._is_accessed == True:
                continue
            # set flag so variable status can be known elsewhere
            var._is_accessed = True
            # add to de-access list to rewind this later
            deaccess_list.append(var)
            # grab numpy object, setting read only if necessary
            var._data = self.dm.getField(var.clean_name).reshape(
                (-1, var.num_components)
            )
            if var not in writeable_vars:
                var._old_data_flag = var._data.flags.writeable
                var._data.flags.writeable = False
            else:
                # increment variable state
                var._increment()
        # if particles moving, update swarm state
        if self.particle_coordinates in writeable_vars:
            self._increment()

        # Create a class which specifies the required context
        # manager hooks (`__enter__`, `__exit__`).
        class exit_manager:
            def __init__(self, swarm):
                self.em_swarm = swarm

            def __enter__(self):
                pass

            def __exit__(self, *args):
                for var in self.em_swarm.vars.values():
                    # only de-access variables we have set access for.
                    if var not in deaccess_list:
                        continue
                    # set this back, although possibly not required.
                    if var not in writeable_vars:
                        var._data.flags.writeable = var._old_data_flag
                    var._data = None
                    self.em_swarm.dm.restoreField(var.clean_name)
                    var._is_accessed = False
                # do particle migration if coords changes
                if self.em_swarm.particle_coordinates in writeable_vars:
                    # let&#39;s use the mesh index to update the particles owning cells.
                    # note that the `petsc4py` interface is more convenient here as the
                    # `SwarmVariable.data` interface is controlled by the context manager
                    # that we are currently within, and it is therefore too easy to
                    # get things wrong that way.
                    cellid = self.em_swarm.dm.getField(&#34;DMSwarm_cellid&#34;)
                    coords = self.em_swarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                        (-1, self.em_swarm.dim)
                    )
                    cellid[:] = self.em_swarm.mesh.get_closest_cells(coords).reshape(-1)
                    self.em_swarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
                    self.em_swarm.dm.restoreField(&#34;DMSwarm_cellid&#34;)
                    # now migrate.
                    self.em_swarm.dm.migrate(remove_sent_points=True)
                    # void these things too
                    self.em_swarm._index = None
                    self.em_swarm._nnmapdict = {}
                # do var updates
                for var in self.em_swarm.vars.values():
                    # if swarm migrated, update all.
                    # if var updated, update var.
                    if (self.em_swarm.particle_coordinates in writeable_vars) or (
                        var in writeable_vars
                    ):
                        var._update()

                uw.timing._decrementDepth()
                uw.timing.log_result(time.time() - stime, &#34;Swarm.access&#34;, 1)

        return exit_manager(self)

    @timing.routine_timer_decorator
    def _get_map(self, var):
        # generate tree if not avaiable
        if not self._index:
            with self.access():
                self._index = uw.kdtree.KDTree(self.data)

        # get or generate map
        meshvar_coords = var._meshVar.coords
        # we can&#39;t use numpy arrays directly as keys in python dicts, so
        # we&#39;ll use `xxhash` to generate a hash of array.
        # this shouldn&#39;t be an issue performance wise but we should test to be
        # sufficiently confident of this.
        import xxhash

        h = xxhash.xxh64()
        h.update(meshvar_coords)
        digest = h.intdigest()
        if digest not in self._nnmapdict:
            self._nnmapdict[digest] = self._index.find_closest_point(meshvar_coords)[0]
        return self._nnmapdict[digest]

    @timing.routine_timer_decorator
    def advection(
        self,
        V_fn,
        delta_t,
        order=2,
        corrector=False,
        restore_points_to_domain_func=None,
    ):

        # X0 holds the particle location at the start of advection
        # This is needed because the particles may be migrated off-proc
        # during timestepping.

        X0 = self._X0

        # Use current velocity to estimate where the particles would have
        # landed in an implicit step.

        # ? how does this interact with the particle restoration function ?

        V_fn_matrix = self.mesh.vector.to_matrix(V_fn)

        if corrector == True and not self._X0_uninitialised:
            with self.access(self.particle_coordinates):
                v_at_Vpts = np.zeros_like(self.data)

                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d], self.data
                    ).reshape(-1)

                corrected_position = X0.data + delta_t * v_at_Vpts
                if restore_points_to_domain_func is not None:
                    corrected_position = restore_points_to_domain_func(
                        corrected_position
                    )

                updated_current_coords = 0.5 * (corrected_position + self.data)

                # validate_coords to ensure they live within the domain (or there will be trouble)

                if restore_points_to_domain_func is not None:
                    updated_current_coords = restore_points_to_domain_func(
                        updated_current_coords
                    )

                self.data[...] = updated_current_coords[...]

        with self.access(X0):
            X0.data[...] = self.data[...]
            self._X0_uninitialised = False

        # Mid point algorithm (2nd order)
        if order == 2:
            with self.access(self.particle_coordinates):

                v_at_Vpts = np.zeros_like(self.data)

                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d], self.data
                    ).reshape(-1)

                mid_pt_coords = self.data[...] + 0.5 * delta_t * v_at_Vpts

                # validate_coords to ensure they live within the domain (or there will be trouble)

                if restore_points_to_domain_func is not None:
                    mid_pt_coords = restore_points_to_domain_func(mid_pt_coords)

                self.data[...] = mid_pt_coords[...]

                ## Let the swarm be updated, and then move the rest of the way

            with self.access(self.particle_coordinates):

                v_at_Vpts = np.zeros_like(self.data)

                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d], self.data
                    ).reshape(-1)

                # if (uw.mpi.rank == 0):
                #     print(&#34;Re-launch from X0&#34;, flush=True)

                new_coords = X0.data[...] + delta_t * v_at_Vpts

                # validate_coords to ensure they live within the domain (or there will be trouble)
                if restore_points_to_domain_func is not None:
                    new_coords = restore_points_to_domain_func(new_coords)

                self.data[...] = new_coords[...]

        # Previous position algorithm (cf above) - we use the previous step as the
        # launch point using the current velocity field. This gives a correction to the previous
        # landing point.

        # assumes X0 is stored from the previous step ... midpoint is needed in the first step

        # forward Euler (1st order)
        else:
            with self.access(self.particle_coordinates):
                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(V_fn[d], self.data).reshape(
                        -1
                    )

                new_coords = self.data + delta_t * v_at_Vpts

                # validate_coords to ensure they live within the domain (or there will be trouble)

                if restore_points_to_domain_func is not None:
                    new_coords = restore_points_to_domain_func(new_coords)

                self.data[...] = new_coords

        ## Cycling of the swarm is a cheap and cheerful version of population control for particles. It turns the
        ## swarm into a streak-swarm where particles are Lagrangian for a number of steps and then reset to their
        ## original location.

        if self.recycle_rate &gt; 1:
            # Restore particles which have cycle == cycle rate (use &gt;= just in case)

            # Remove remesh points and recreate a new set at the mesh-local
            # locations that we already have stored.

            with self.access(self.particle_coordinates, self._remeshed):
                remeshed = self._remeshed.data[:, 0] == 0
                # This is one way to do it ... we can do this better though
                self.data[remeshed, 0] = 1.0e100

            swarm_size = self.dm.getLocalSize()

            num_remeshed_points = self.mesh.particle_X_orig.shape[0]

            self.dm.addNPoints(num_remeshed_points)

            cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
            coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
            rmsh = self.dm.getField(&#34;DMSwarm_remeshed&#34;)

            # print(f&#34;cellid -&gt; {cellid.shape}&#34;)
            # print(f&#34;particle coords -&gt; {coords.shape}&#34;)
            # print(f&#34;remeshed points  -&gt; {num_remeshed_points}&#34;)

            perturbation = (
                (0.33 / (1 + self.fill_param))
                * (np.random.random(size=(num_remeshed_points, self.dim)) - 0.5)
                * self.mesh._radii[cellid[swarm_size::]].reshape(-1, 1)
            )

            # print(f&#34;{perturbation}&#34;)

            coords[swarm_size::] = self.mesh.particle_X_orig[:, :] + perturbation
            cellid[swarm_size::] = self.mesh.particle_CellID_orig[:, 0]
            rmsh[swarm_size::] = 0

            self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
            self.dm.restoreField(&#34;DMSwarm_remeshed&#34;)

            # when we let this go, the particles may be re-distributed to
            # other processors, and we will need to rebuild the remeshed
            # array before trying to compute / assign values to variables

            for swarmVar in self.vars.values():
                if swarmVar._rebuild_on_cycle:
                    with self.access(swarmVar):

                        if swarmVar.dtype is int:
                            nnn = 1
                        else:
                            nnn = self.mesh.dim + 1  # 3 for triangles, 4 for tets ...

                        interpolated_values = (
                            swarmVar.rbf_interpolate(self.mesh.particle_X_orig, nnn=nnn)
                            # uw.function.evaluate(swarmVar._meshVar.fn, remeshed_coords)
                        ).astype(swarmVar.dtype)

                        swarmVar.data[swarm_size::] = interpolated_values

            self.dm.migrate(remove_sent_points=True)

            with self.access(self._remeshed):
                self._remeshed.data[...] = np.mod(
                    self._remeshed.data[...] - 1, self.recycle_rate
                )

            self.cycle += 1

        return</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>underworld3.utilities._api_tools.Stateful</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="underworld3.swarm.Swarm.instances"><code class="name">var <span class="ident">instances</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="underworld3.swarm.Swarm.data"><code class="name">var <span class="ident">data</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def data(self):
    return self.particle_coordinates.data</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.mesh"><code class="name">var <span class="ident">mesh</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mesh(self):
    return self._mesh</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.particle_cellid"><code class="name">var <span class="ident">particle_cellid</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def particle_cellid(self):
    return self._cellid_var</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.particle_coordinates"><code class="name">var <span class="ident">particle_coordinates</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def particle_coordinates(self):
    return self._coord_var</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.vars"><code class="name">var <span class="ident">vars</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def vars(self):
    return self._vars</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="underworld3.swarm.Swarm.access"><code class="name flex">
<span>def <span class="ident">access</span></span>(<span>self, *writeable_vars: <a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>This context manager makes the underlying swarm variables data available to
the user. The data should be accessed via the variables <code>data</code> handle.</p>
<p>As default, all data is read-only. To enable writeable data, the user should
specify which variable they wish to modify.</p>
<p>At the conclusion of the users context managed block, numerous further operations
will be automatically executed. This includes swarm parallel migration routines
where the swarm's <code>particle_coordinates</code> variable has been modified. The swarm
variable proxy mesh variables will also be updated for modifed swarm variables.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>writeable_vars</code></strong></dt>
<dd>The variables for which data write access is required.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python">&gt;&gt;&gt; import underworld3 as uw
&gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
&gt;&gt;&gt; with someMesh.deform_mesh():
...     someMesh.data[0] = [0.1,0.1]
&gt;&gt;&gt; someMesh.data[0]
array([ 0.1,  0.1])
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def access(self, *writeable_vars: SwarmVariable):
    &#34;&#34;&#34;
    This context manager makes the underlying swarm variables data available to
    the user. The data should be accessed via the variables `data` handle.

    As default, all data is read-only. To enable writeable data, the user should
    specify which variable they wish to modify.

    At the conclusion of the users context managed block, numerous further operations
    will be automatically executed. This includes swarm parallel migration routines
    where the swarm&#39;s `particle_coordinates` variable has been modified. The swarm
    variable proxy mesh variables will also be updated for modifed swarm variables.

    Parameters
    ----------
    writeable_vars
        The variables for which data write access is required.

    Example
    -------

    &gt;&gt;&gt; import underworld3 as uw
    &gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
    &gt;&gt;&gt; with someMesh.deform_mesh():
    ...     someMesh.data[0] = [0.1,0.1]
    &gt;&gt;&gt; someMesh.data[0]
    array([ 0.1,  0.1])
    &#34;&#34;&#34;
    import time

    uw.timing._incrementDepth()
    stime = time.time()

    deaccess_list = []
    for var in self._vars.values():
        # if already accessed within higher level context manager, continue.
        if var._is_accessed == True:
            continue
        # set flag so variable status can be known elsewhere
        var._is_accessed = True
        # add to de-access list to rewind this later
        deaccess_list.append(var)
        # grab numpy object, setting read only if necessary
        var._data = self.dm.getField(var.clean_name).reshape(
            (-1, var.num_components)
        )
        if var not in writeable_vars:
            var._old_data_flag = var._data.flags.writeable
            var._data.flags.writeable = False
        else:
            # increment variable state
            var._increment()
    # if particles moving, update swarm state
    if self.particle_coordinates in writeable_vars:
        self._increment()

    # Create a class which specifies the required context
    # manager hooks (`__enter__`, `__exit__`).
    class exit_manager:
        def __init__(self, swarm):
            self.em_swarm = swarm

        def __enter__(self):
            pass

        def __exit__(self, *args):
            for var in self.em_swarm.vars.values():
                # only de-access variables we have set access for.
                if var not in deaccess_list:
                    continue
                # set this back, although possibly not required.
                if var not in writeable_vars:
                    var._data.flags.writeable = var._old_data_flag
                var._data = None
                self.em_swarm.dm.restoreField(var.clean_name)
                var._is_accessed = False
            # do particle migration if coords changes
            if self.em_swarm.particle_coordinates in writeable_vars:
                # let&#39;s use the mesh index to update the particles owning cells.
                # note that the `petsc4py` interface is more convenient here as the
                # `SwarmVariable.data` interface is controlled by the context manager
                # that we are currently within, and it is therefore too easy to
                # get things wrong that way.
                cellid = self.em_swarm.dm.getField(&#34;DMSwarm_cellid&#34;)
                coords = self.em_swarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                    (-1, self.em_swarm.dim)
                )
                cellid[:] = self.em_swarm.mesh.get_closest_cells(coords).reshape(-1)
                self.em_swarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
                self.em_swarm.dm.restoreField(&#34;DMSwarm_cellid&#34;)
                # now migrate.
                self.em_swarm.dm.migrate(remove_sent_points=True)
                # void these things too
                self.em_swarm._index = None
                self.em_swarm._nnmapdict = {}
            # do var updates
            for var in self.em_swarm.vars.values():
                # if swarm migrated, update all.
                # if var updated, update var.
                if (self.em_swarm.particle_coordinates in writeable_vars) or (
                    var in writeable_vars
                ):
                    var._update()

            uw.timing._decrementDepth()
            uw.timing.log_result(time.time() - stime, &#34;Swarm.access&#34;, 1)

    return exit_manager(self)</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.add_particles_with_coordinates"><code class="name flex">
<span>def <span class="ident">add_particles_with_coordinates</span></span>(<span>self, coordinatesArray)</span>
</code></dt>
<dd>
<div class="desc"><p>This method adds particles to the swarm using particle coordinates provided
using a numpy array.
Note that particles with coordinates NOT local to the current processor will
be reject/ignored. Either include an array with all coordinates to all processors
or an array with the local coordinates.
Parameters</p>
<hr>
<dl>
<dt><strong><code>coordinatesArray</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The numpy array containing the coordinate of the new particles. Array is
expected to take shape n*dim, where n is the number of new particles, and
dim is the dimensionality of the swarm's supporting mesh.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def add_particles_with_coordinates(self, coordinatesArray):
    &#34;&#34;&#34;
    This method adds particles to the swarm using particle coordinates provided
    using a numpy array.
    Note that particles with coordinates NOT local to the current processor will
    be reject/ignored. Either include an array with all coordinates to all processors
    or an array with the local coordinates.
    Parameters
    ----------
    coordinatesArray : numpy.ndarray
        The numpy array containing the coordinate of the new particles. Array is
        expected to take shape n*dim, where n is the number of new particles, and
        dim is the dimensionality of the swarm&#39;s supporting mesh.
    &#34;&#34;&#34;

    if not isinstance(coordinatesArray, np.ndarray):
        raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
    if not len(coordinatesArray.shape) == 2:
        raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
    if not coordinatesArray.shape[1] == self.mesh.dim:
        #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
        raise ValueError(
            &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                          number of particles to add, and &#39;dim&#39; is the dimensionality of
                          the supporting mesh ({}).&#34;&#34;&#34;.format(
                self.mesh.dim
            )
        )

    cells = self.mesh.get_closest_local_cells(coordinatesArray)

    valid_coordinates = coordinatesArray[cells != -1]
    valid_cells = cells[cells != -1]

    npoints = len(valid_coordinates)
    swarm_size = self.dm.getLocalSize()

    # -1 means no particles have been added yet
    if swarm_size == -1:
        swarm_size = 0
        npoints = npoints + 1

    self.dm.finalizeFieldRegister()
    self.dm.addNPoints(npoints=npoints)

    cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

    coords[swarm_size::, :] = valid_coordinates[:, :]
    cellid[swarm_size::] = valid_cells[:]

    self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
    self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

    # Here we update the swarm cycle values as required

    if self.recycle_rate &gt; 1:
        with self.access(self._remeshed):
            # self._Xorig.data[...] = coordinatesArray
            self._remeshed.data[...] = 0

    self.dm.migrate(remove_sent_points=True)

    return</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.add_variable"><code class="name flex">
<span>def <span class="ident">add_variable</span></span>(<span>self, name, size=1, dtype=builtins.float, proxy_degree=2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def add_variable(
    self,
    name,
    size=1,
    dtype=float,
    proxy_degree=2,
    _nn_proxy=False,
):

    return SwarmVariable(
        name,
        self,
        size,
        dtype=dtype,
        proxy_degree=proxy_degree,
        _nn_proxy=_nn_proxy,
    )</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.advection"><code class="name flex">
<span>def <span class="ident">advection</span></span>(<span>self, V_fn, delta_t, order=2, corrector=False, restore_points_to_domain_func=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def advection(
    self,
    V_fn,
    delta_t,
    order=2,
    corrector=False,
    restore_points_to_domain_func=None,
):

    # X0 holds the particle location at the start of advection
    # This is needed because the particles may be migrated off-proc
    # during timestepping.

    X0 = self._X0

    # Use current velocity to estimate where the particles would have
    # landed in an implicit step.

    # ? how does this interact with the particle restoration function ?

    V_fn_matrix = self.mesh.vector.to_matrix(V_fn)

    if corrector == True and not self._X0_uninitialised:
        with self.access(self.particle_coordinates):
            v_at_Vpts = np.zeros_like(self.data)

            for d in range(self.dim):
                v_at_Vpts[:, d] = uw.function.evaluate(
                    V_fn_matrix[d], self.data
                ).reshape(-1)

            corrected_position = X0.data + delta_t * v_at_Vpts
            if restore_points_to_domain_func is not None:
                corrected_position = restore_points_to_domain_func(
                    corrected_position
                )

            updated_current_coords = 0.5 * (corrected_position + self.data)

            # validate_coords to ensure they live within the domain (or there will be trouble)

            if restore_points_to_domain_func is not None:
                updated_current_coords = restore_points_to_domain_func(
                    updated_current_coords
                )

            self.data[...] = updated_current_coords[...]

    with self.access(X0):
        X0.data[...] = self.data[...]
        self._X0_uninitialised = False

    # Mid point algorithm (2nd order)
    if order == 2:
        with self.access(self.particle_coordinates):

            v_at_Vpts = np.zeros_like(self.data)

            for d in range(self.dim):
                v_at_Vpts[:, d] = uw.function.evaluate(
                    V_fn_matrix[d], self.data
                ).reshape(-1)

            mid_pt_coords = self.data[...] + 0.5 * delta_t * v_at_Vpts

            # validate_coords to ensure they live within the domain (or there will be trouble)

            if restore_points_to_domain_func is not None:
                mid_pt_coords = restore_points_to_domain_func(mid_pt_coords)

            self.data[...] = mid_pt_coords[...]

            ## Let the swarm be updated, and then move the rest of the way

        with self.access(self.particle_coordinates):

            v_at_Vpts = np.zeros_like(self.data)

            for d in range(self.dim):
                v_at_Vpts[:, d] = uw.function.evaluate(
                    V_fn_matrix[d], self.data
                ).reshape(-1)

            # if (uw.mpi.rank == 0):
            #     print(&#34;Re-launch from X0&#34;, flush=True)

            new_coords = X0.data[...] + delta_t * v_at_Vpts

            # validate_coords to ensure they live within the domain (or there will be trouble)
            if restore_points_to_domain_func is not None:
                new_coords = restore_points_to_domain_func(new_coords)

            self.data[...] = new_coords[...]

    # Previous position algorithm (cf above) - we use the previous step as the
    # launch point using the current velocity field. This gives a correction to the previous
    # landing point.

    # assumes X0 is stored from the previous step ... midpoint is needed in the first step

    # forward Euler (1st order)
    else:
        with self.access(self.particle_coordinates):
            for d in range(self.dim):
                v_at_Vpts[:, d] = uw.function.evaluate(V_fn[d], self.data).reshape(
                    -1
                )

            new_coords = self.data + delta_t * v_at_Vpts

            # validate_coords to ensure they live within the domain (or there will be trouble)

            if restore_points_to_domain_func is not None:
                new_coords = restore_points_to_domain_func(new_coords)

            self.data[...] = new_coords

    ## Cycling of the swarm is a cheap and cheerful version of population control for particles. It turns the
    ## swarm into a streak-swarm where particles are Lagrangian for a number of steps and then reset to their
    ## original location.

    if self.recycle_rate &gt; 1:
        # Restore particles which have cycle == cycle rate (use &gt;= just in case)

        # Remove remesh points and recreate a new set at the mesh-local
        # locations that we already have stored.

        with self.access(self.particle_coordinates, self._remeshed):
            remeshed = self._remeshed.data[:, 0] == 0
            # This is one way to do it ... we can do this better though
            self.data[remeshed, 0] = 1.0e100

        swarm_size = self.dm.getLocalSize()

        num_remeshed_points = self.mesh.particle_X_orig.shape[0]

        self.dm.addNPoints(num_remeshed_points)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
        rmsh = self.dm.getField(&#34;DMSwarm_remeshed&#34;)

        # print(f&#34;cellid -&gt; {cellid.shape}&#34;)
        # print(f&#34;particle coords -&gt; {coords.shape}&#34;)
        # print(f&#34;remeshed points  -&gt; {num_remeshed_points}&#34;)

        perturbation = (
            (0.33 / (1 + self.fill_param))
            * (np.random.random(size=(num_remeshed_points, self.dim)) - 0.5)
            * self.mesh._radii[cellid[swarm_size::]].reshape(-1, 1)
        )

        # print(f&#34;{perturbation}&#34;)

        coords[swarm_size::] = self.mesh.particle_X_orig[:, :] + perturbation
        cellid[swarm_size::] = self.mesh.particle_CellID_orig[:, 0]
        rmsh[swarm_size::] = 0

        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_remeshed&#34;)

        # when we let this go, the particles may be re-distributed to
        # other processors, and we will need to rebuild the remeshed
        # array before trying to compute / assign values to variables

        for swarmVar in self.vars.values():
            if swarmVar._rebuild_on_cycle:
                with self.access(swarmVar):

                    if swarmVar.dtype is int:
                        nnn = 1
                    else:
                        nnn = self.mesh.dim + 1  # 3 for triangles, 4 for tets ...

                    interpolated_values = (
                        swarmVar.rbf_interpolate(self.mesh.particle_X_orig, nnn=nnn)
                        # uw.function.evaluate(swarmVar._meshVar.fn, remeshed_coords)
                    ).astype(swarmVar.dtype)

                    swarmVar.data[swarm_size::] = interpolated_values

        self.dm.migrate(remove_sent_points=True)

        with self.access(self._remeshed):
            self._remeshed.data[...] = np.mod(
                self._remeshed.data[...] - 1, self.recycle_rate
            )

        self.cycle += 1

    return</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def load(
    self,
    filename: str,
):
    ### open up file with coords on all procs
    with h5py.File(f&#34;{filename}&#34;, &#34;r&#34;) as h5f:
        coordinates = h5f[&#34;coordinates&#34;][:]

    #### utilises the UW function for adding a swarm by an array
    self.add_particles_with_coordinates(coordinates)

    return</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.petsc_save_checkpoint"><code class="name flex">
<span>def <span class="ident">petsc_save_checkpoint</span></span>(<span>self, swarmName: str, index: int, outputPath: Optional[str] = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Use PETSc to save the swarm and attached data to a .pbin and xdmf file.</p>
<h2 id="parameters">Parameters</h2>
<p>swarmName :
Name of the swarm to save.
index :
An index which might correspond to the timestep or output number (for example).
outputPath :
Path to save the data. If left empty it will save the data in the current working directory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def petsc_save_checkpoint(
    self,
    swarmName: str,
    index: int,
    outputPath: Optional[str] = &#34;&#34;,
):

    &#34;&#34;&#34;

    Use PETSc to save the swarm and attached data to a .pbin and xdmf file.

    Parameters
    ----------
    swarmName :
        Name of the swarm to save.
    index :
        An index which might correspond to the timestep or output number (for example).
    outputPath :
        Path to save the data. If left empty it will save the data in the current working directory.
    &#34;&#34;&#34;

    x_swarm_fname = f&#34;{outputPath}{swarmName}_{index:04d}.xmf&#34;
    self.dm.viewXDMF(x_swarm_fname)</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.populate"><code class="name flex">
<span>def <span class="ident">populate</span></span>(<span>self, fill_param: Optional[int] = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Populate the swarm with particles throughout the domain.</p>
<h2 id="parameters">Parameters</h2>
<p>fill_param:
Parameter determining the particle count per cell (per dimension)
for the given layout, using the mesh degree.</p>
<p>cell_search:
Use k-d tree to locate nearest cells (fails if this swarm is used to build a k-d tree)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def populate(
    self,
    fill_param: Optional[int] = 1,
):
    (
        &#34;&#34;&#34;
    Populate the swarm with particles throughout the domain.

    Parameters
    ----------
    fill_param:
        Parameter determining the particle count per cell (per dimension)
        for the given layout, using the mesh degree.

    cell_search:
        Use k-d tree to locate nearest cells (fails if this swarm is used to build a k-d tree)

    &#34;&#34;&#34;
    )

    self.fill_param = fill_param

    newp_coords0 = self.mesh._get_coords_for_basis(fill_param, continuous=False)
    newp_cells0 = self.mesh.get_closest_local_cells(newp_coords0)

    valid = newp_cells0 != -1
    newp_coords = newp_coords0[valid]
    newp_cells = newp_cells0[valid]

    self.dm.finalizeFieldRegister()
    self.dm.addNPoints(newp_coords.shape[0] + 1)

    cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

    coords[...] = newp_coords[...]
    cellid[:] = newp_cells[:]

    self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
    self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

    ## Now make a series of copies to allow the swarm cycling to
    ## work correctly (if required)

    # cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    # lost = np.where(cellid == -1)
    # print(f&#34;{uw.mpi.rank} - lost particles: {lost[0].shape} out of {cellid.shape}&#34;, flush=True)
    # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

    if self.recycle_rate &gt; 1:
        with self.access():
            # Actually, this is a mesh-local quantity, so let&#39;s just
            # store it on the mesh in an ad_hoc fashion for now

            self.mesh.particle_X_orig = self.particle_coordinates.data.copy()
            self.mesh.particle_CellID_orig = self._cellid_var.data.copy()

        with self.access():
            swarm_orig_size = self.particle_coordinates.data.shape[0]
            all_local_coords = np.vstack(
                (self.particle_coordinates.data,) * (self.recycle_rate)
            )
            all_local_cells = np.vstack(
                (self._cellid_var.data,) * (self.recycle_rate)
            )

            swarm_new_size = all_local_coords.data.shape[0]

        self.dm.addNPoints(swarm_new_size - swarm_orig_size)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[...] = (
            all_local_coords[...]
            + (0.33 / (1 + fill_param))
            * (np.random.random(size=all_local_coords.shape) - 0.5)
            * self.mesh._search_lengths[all_local_cells]  # typical cell size
        )
        cellid[:] = all_local_cells[:, 0]

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        ## Now set the cycle values

        with self.access(self._remeshed):
            for i in range(0, self.recycle_rate):
                offset = swarm_orig_size * i
                self._remeshed.data[offset::, 0] = i

    # Validate (eliminate if required)

    # cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    # lost = np.where(cellid == -1)
    # print(f&#34;{uw.mpi.rank} - lost particles: {lost[0].shape} out of {cellid.shape}&#34;, flush=True)
    # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

    return</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filename: int, compression: Optional[bool] = False, compressionType: Optional[str] = 'gzip', force_sequential=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the swarm coordinates to a h5 file.</p>
<h2 id="parameters">Parameters</h2>
<p>filename :
The filename of the swarm checkpoint file to save to disk.
compression :
Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
compressionType :
Type of compression to use, 'gzip' and 'lzf' supported. 'gzip' is default. Compression also needs to be set to 'True'.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def save(
    self,
    filename: int,
    compression: Optional[bool] = False,
    compressionType: Optional[str] = &#34;gzip&#34;,
    force_sequential=False,
):
    &#34;&#34;&#34;

    Save the swarm coordinates to a h5 file.

    Parameters
    ----------
    filename :
        The filename of the swarm checkpoint file to save to disk.
    compression :
        Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
    compressionType :
        Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.



    &#34;&#34;&#34;
    if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
        warnings.warn(
            &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
            stacklevel=2,
        )
    if filename.endswith(&#34;.h5&#34;) == False:
        raise RuntimeError(&#34;The filename must end with .h5&#34;)
    if compression == True and comm.rank == 0:
        warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)

    if h5py.h5.get_config().mpi == True and not force_sequential:

        # It seems to be a bad idea to mix mpi barriers with the access
        # context manager so the copy-free version of this seems to hang
        # when there are many active cores. This is probably why the parallel
        # h5py write hangs

        with self.access():
            data_copy = self.data[:].copy()

        with h5py.File(
            f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=MPI.COMM_WORLD
        ) as h5f:
            if compression == True:
                h5f.create_dataset(
                    &#34;coordinates&#34;,
                    data=data_copy[:],
                    compression=compressionType,
                )
            else:
                h5f.create_dataset(&#34;coordinates&#34;, data=data_copy[:])
    else:

        # It seems to be a bad idea to mix mpi barriers with the access
        # context manager so the copy-free version of this seems to hang
        # when there are many active cores

        with self.access():
            data_copy = self.data[:].copy()

        if comm.rank == 0:
            with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                if compression == True:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy,
                        chunks=True,
                        maxshape=(None, data_copy.shape[1]),
                        compression=compressionType,
                    )
                else:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy,
                        chunks=True,
                        maxshape=(None, data_copy.shape[1]),
                    )

        comm.barrier()
        for i in range(1, comm.size):
            if comm.rank == i:
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                    h5f[&#34;coordinates&#34;].resize(
                        (h5f[&#34;coordinates&#34;].shape[0] + data_copy.shape[0]),
                        axis=0,
                    )
                    # passive swarm, zero local particles is not unusual
                    if data_copy.shape[0] &gt; 0:
                        h5f[&#34;coordinates&#34;][-data_copy.shape[0] :] = data_copy[:]
            comm.barrier()
        comm.barrier()

    return</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.Swarm.save_checkpoint"><code class="name flex">
<span>def <span class="ident">save_checkpoint</span></span>(<span>self, swarmName: str, swarmVars: list, index: int, outputPath: Optional[str] = '', time: Optional[int] = None, compression: Optional[bool] = False, compressionType: Optional[str] = 'gzip', force_sequential: Optional[bool] = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Save data to h5 and a corresponding xdmf for visualisation using h5py.</p>
<h2 id="parameters">Parameters</h2>
<p>swarmName :
Name of the swarm to save.
swarmVars :
List of swarm objects to save.
index :
An index which might correspond to the timestep or output number (for example).
outputPath :
Path to save the data. If left empty it will save the data in the current working directory.
time :
Attach the time to the generated xdmf.
compression :
Whether to compress the h5 files [bool].
compressionType :
The type of compression to use. 'gzip' and 'lzf' are the supported types, with 'gzip' as the default.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def save_checkpoint(
    self,
    swarmName: str,
    swarmVars: list,
    index: int,
    outputPath: Optional[str] = &#34;&#34;,
    time: Optional[int] = None,
    compression: Optional[bool] = False,
    compressionType: Optional[str] = &#34;gzip&#34;,
    force_sequential: Optional[bool] = False,
):

    &#34;&#34;&#34;

    Save data to h5 and a corresponding xdmf for visualisation using h5py.

    Parameters
    ----------
    swarmName :
        Name of the swarm to save.
    swarmVars :
        List of swarm objects to save.
    index :
        An index which might correspond to the timestep or output number (for example).
    outputPath :
        Path to save the data. If left empty it will save the data in the current working directory.
    time :
        Attach the time to the generated xdmf.
    compression :
        Whether to compress the h5 files [bool].
    compressionType :
        The type of compression to use. &#39;gzip&#39; and &#39;lzf&#39; are the supported types, with &#39;gzip&#39; as the default.
    &#34;&#34;&#34;

    if swarmVars != None and not isinstance(swarmVars, list):
        raise RuntimeError(&#34;`swarmVars` does not appear to be a list.&#34;)

    else:
        ### save the swarm particle location
        self.save(
            filename=f&#34;{outputPath}{swarmName}-{index:04d}.h5&#34;,
            compression=compression,
            compressionType=compressionType,
            force_sequential=force_sequential,
        )

    #### Generate a h5 file for each field
    if swarmVars != None:
        for field in swarmVars:
            field.save(
                filename=f&#34;{outputPath}{field.name}-{index:04d}.h5&#34;,
                compression=compression,
                compressionType=compressionType,
                force_sequential=force_sequential,
            )

    if uw.mpi.rank == 0:
        ### only need to combine the h5 files to a single xdmf on one proc
        with open(f&#34;{outputPath}{swarmName}-{index:04d}.xmf&#34;, &#34;w&#34;) as xdmf:
            # Write the XDMF header
            xdmf.write(&#39;&lt;?xml version=&#34;1.0&#34; ?&gt;\n&#39;)
            xdmf.write(
                &#39;&lt;Xdmf xmlns:xi=&#34;http://www.w3.org/2001/XInclude&#34; Version=&#34;2.0&#34;&gt;\n&#39;
            )
            xdmf.write(&#34;&lt;Domain&gt;\n&#34;)
            xdmf.write(
                f&#39;&lt;Grid Name=&#34;{swarmName}-{index:04d}&#34; GridType=&#34;Uniform&#34;&gt;\n&#39;
            )

            if time != None:
                xdmf.write(f&#39;       &lt;Time Value=&#34;{time}&#34; /&gt;\n&#39;)

            # Write the grid element for the HDF5 dataset
            with h5py.File(f&#34;{outputPath}{swarmName}-{index:04}.h5&#34;, &#34;r&#34;) as h5f:
                xdmf.write(
                    f&#39;      &lt;Topology Type=&#34;POLYVERTEX&#34; NodesPerElement=&#34;{h5f[&#34;coordinates&#34;].shape[0]}&#34;&gt; &lt;/Topology&gt;\n&#39;
                )
                if h5f[&#34;coordinates&#34;].shape[1] == 2:
                    xdmf.write(&#39;            &lt;Geometry Type=&#34;XY&#34;&gt;\n&#39;)
                elif h5f[&#34;coordinates&#34;].shape[1] == 3:
                    xdmf.write(&#39;            &lt;Geometry Type=&#34;XYZ&#34;&gt;\n&#39;)
                xdmf.write(
                    f&#39;                      &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;coordinates&#34;].shape[0]} {h5f[&#34;coordinates&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/coordinates&lt;/DataItem&gt;\n&#39;
                )
                xdmf.write(&#34;                &lt;/Geometry&gt;\n&#34;)

            # Write the attribute element for the field
            if swarmVars != None:
                for field in swarmVars:
                    with h5py.File(
                        f&#34;{outputPath}{field.name}-{index:04d}.h5&#34;, &#34;r&#34;
                    ) as h5f:
                        if h5f[&#34;data&#34;].dtype == np.int32:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Int&#34; Precision=&#34;4&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        elif h5f[&#34;data&#34;].shape[1] == 1:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        elif h5f[&#34;data&#34;].shape[1] == 2 or h5f[&#34;data&#34;].shape[1] == 3:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Vector&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        else:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Tensor&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )

                        xdmf.write(&#34;        &lt;/Attribute&gt;\n&#34;)
            else:
                pass

            # Write the XDMF footer
            xdmf.write(&#34;&lt;/Grid&gt;\n&#34;)
            xdmf.write(&#34;&lt;/Domain&gt;\n&#34;)
            xdmf.write(&#34;&lt;/Xdmf&gt;\n&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="underworld3.swarm.SwarmPICLayout"><code class="flex name class">
<span>class <span class="ident">SwarmPICLayout</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Particle population fill type:</p>
<p>SwarmPICLayout.REGULAR
defines points on a regular ijk mesh. Supported by simplex cell types only.
SwarmPICLayout.GAUSS
defines points using an npoint Gauss-Legendre tensor product quadrature rule.
SwarmPICLayout.SUBDIVISION defines points on the centroid of a sub-divided reference cell.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SwarmPICLayout(Enum):
    &#34;&#34;&#34;
    Particle population fill type:

    SwarmPICLayout.REGULAR     defines points on a regular ijk mesh. Supported by simplex cell types only.
    SwarmPICLayout.GAUSS       defines points using an npoint Gauss-Legendre tensor product quadrature rule.
    SwarmPICLayout.SUBDIVISION defines points on the centroid of a sub-divided reference cell.
    &#34;&#34;&#34;

    REGULAR = 0
    GAUSS = 1
    SUBDIVISION = 2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="underworld3.swarm.SwarmPICLayout.GAUSS"><code class="name">var <span class="ident">GAUSS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmPICLayout.REGULAR"><code class="name">var <span class="ident">REGULAR</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmPICLayout.SUBDIVISION"><code class="name">var <span class="ident">SUBDIVISION</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="underworld3.swarm.SwarmType"><code class="flex name class">
<span>class <span class="ident">SwarmType</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>An enumeration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SwarmType(Enum):
    DMSWARM_PIC = 1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="underworld3.swarm.SwarmType.DMSWARM_PIC"><code class="name">var <span class="ident">DMSWARM_PIC</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="underworld3.swarm.SwarmVariable"><code class="flex name class">
<span>class <span class="ident">SwarmVariable</span></span>
<span>(</span><span>name, swarm, size, vtype=None, dtype=builtins.float, proxy_degree=1, proxy_continuous=True, varsymbol=None, rebuild_on_cycle=True)</span>
</code></dt>
<dd>
<div class="desc"><p>This is a mixin class for underworld objects that are stateful.
The state of an object is incremented whenever it is modified.
For example, heavy variables have states, and when a user modifies
it within its <code>access()</code> context manager, its state is incremented
at the conclusion of their modifications.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SwarmVariable(_api_tools.Stateful):
    @timing.routine_timer_decorator
    def __init__(
        self,
        name,
        swarm,
        size,  # only needed if MATRIX type
        vtype=None,
        dtype=float,
        proxy_degree=1,
        proxy_continuous=True,
        _register=True,
        _proxy=True,
        _nn_proxy=False,
        varsymbol=None,
        rebuild_on_cycle=True,
    ):

        if name in swarm.vars.keys():
            raise ValueError(
                &#34;Variable with name {} already exists on swarm.&#34;.format(name)
            )

        import re
        import sympy
        import math

        if varsymbol is None:
            varsymbol = name

        self.name = name
        self.clean_name = re.sub(r&#34;[^a-zA-Z0-9_]&#34;, &#34;&#34;, name)
        self.symbol = varsymbol

        self.swarm = swarm
        self.shape = size

        mesh = swarm.mesh

        if vtype == None:
            if isinstance(size, int) and size == 1:
                vtype = uw.VarType.SCALAR
            elif isinstance(size, int) and size == mesh.dim:
                vtype = uw.VarType.VECTOR
            elif isinstance(size, tuple):
                if size[0] == mesh.dim and size[1] == mesh.dim:
                    vtype = uw.VarType.TENSOR
                else:
                    vtype = uw.VarType.MATRIX
            else:
                raise ValueError(
                    &#34;Unable to infer variable type from `num_components`. Please explicitly set the `vtype` parameter.&#34;
                )

        if not isinstance(vtype, uw.VarType):
            raise ValueError(
                &#34;&#39;vtype&#39; must be an instance of &#39;Variable_Type&#39;, for example `underworld.VarType.SCALAR`.&#34;
            )

        if vtype == uw.VarType.SCALAR:
            self.num_components = 1
            self.shape = 1
            self.cpt_map = 0
        elif vtype == uw.VarType.VECTOR:
            self.num_components = mesh.dim
            self.shape = mesh.dim
            self.cpt_map = tuple(range(0, mesh.dim))
        elif vtype == uw.VarType.TENSOR:
            self.num_components = mesh.dim * mesh.dim
            self.shape = (mesh.dim, mesh.dim)
        elif vtype == uw.VarType.SYM_TENSOR:
            self.num_components = math.comb(mesh.dim + 1, 2)
            self.shape = (mesh.dim, mesh.dim)
        elif vtype == uw.VarType.MATRIX:
            self.num_components = self.shape[0] * self.shape[1]

        if (dtype == float) or (dtype == &#34;float&#34;) or (dtype == np.float64):
            self.dtype = float
            petsc_type = PETSc.ScalarType
        elif (
            (dtype == int)
            or (dtype == &#34;int&#34;)
            or (dtype == np.int32)
            or (dtype == np.int64)
        ):
            self.dtype = int
            petsc_type = PETSc.IntType
        else:
            raise TypeError(
                f&#34;Provided dtype={dtype} is not supported. Supported types are &#39;int&#39; and &#39;float&#39;.&#34;
            )

        if _register:
            self.swarm.dm.registerField(
                self.clean_name, self.num_components, dtype=petsc_type
            )

        self._data = None
        # add to swarms dict

        self.swarm._vars[self.clean_name] = self
        self._is_accessed = False

        # proxy variable
        self._proxy = _proxy
        self._vtype = vtype
        self._proxy_degree = proxy_degree
        self._proxy_continuous = proxy_continuous
        self._nn_proxy = _nn_proxy
        self._create_proxy_variable()

        # recycle swarm
        self._rebuild_on_cycle = rebuild_on_cycle

        self._register = _register

        super().__init__()

        return

    def _create_proxy_variable(self):

        # release if defined
        self._meshVar = None

        if self._proxy:
            self._meshVar = uw.discretisation.MeshVariable(
                &#34;proxy_&#34; + self.clean_name,
                self.swarm._mesh,
                self.shape,
                self._vtype,
                degree=self._proxy_degree,
                continuous=self._proxy_continuous,
                varsymbol=r&#34;\left&lt;&#34; + self.symbol + r&#34;\right&gt;&#34;,
            )

    def _update(self):
        &#34;&#34;&#34;
        This method updates the proxy mesh variable for the current
        swarm &amp; particle variable state.
        &#34;&#34;&#34;

        # if not proxied, nothing to do. return.
        if not self._meshVar:
            return

        else:
            self._rbf_to_meshVar(self._meshVar)

        return

    # Maybe rbf_interpolate for this one and meshVar is a special case
    def _rbf_to_meshVar(self, meshVar, nnn=None, verbose=False):
        &#34;&#34;&#34;
        Here is how it works: for each particle, create a distance-weighted average on the node data

        Todo: caching the k-d trees etc for the proxy-mesh-variable nodal points
        Todo: some form of global fall-back for when there are no particles on a processor
        &#34;&#34;&#34;

        # Mapping to the coordinates of the variable from the
        # particle coords

        if nnn is None:
            nnn = self.swarm.mesh.dim + 1

        if meshVar.mesh != self.swarm.mesh:
            raise RuntimeError(&#34;Cannot map a swarm to a different mesh&#34;)

        new_coords = meshVar.coords

        Values = self.rbf_interpolate(new_coords, verbose=verbose, nnn=nnn)

        with meshVar.mesh.access(meshVar):
            meshVar.data[...] = Values[...]

        return

    def rbf_interpolate(self, new_coords, verbose=False, nnn=None):

        # An inverse-distance mapping is quite robust here ... as long
        # as long we take care of the case where some nodes coincide (likely if used mesh2mesh)
        # We try to eliminate contributions from recently remeshed particles

        import numpy as np

        if nnn is None:
            nnn = self.swarm.mesh.dim + 1

        with self.swarm.access():
            if self.swarm.recycle_rate &gt; 1:
                not_remeshed = self.swarm._remeshed.data[:, 0] != 0
                D = self.data[not_remeshed].copy()

                kdt = uw.kdtree.KDTree(
                    self.swarm.particle_coordinates.data[not_remeshed, :]
                )
            else:
                D = self.data.copy()
                kdt = uw.kdtree.KDTree(self.swarm.particle_coordinates.data[:, :])

            kdt.build_index()

        return kdt.rbf_interpolator_local(new_coords, D, nnn, verbose)

    # ToDo: I don&#39;t think this is used / up to date
    @timing.routine_timer_decorator
    def project_from(self, meshvar):
        # use method found in
        # /tmp/petsc-build/petsc/src/dm/impls/swarm/tests/ex2.c
        # to project from fields to particles

        self.swarm.mesh.dm.clearDS()
        self.swarm.mesh.dm.createDS()

        meshdm = meshvar.mesh.dm
        fields = meshvar.field_id
        _, meshvardm = meshdm.createSubDM(fields)

        ksp = PETSc.KSP().create()
        ksp.setOptionsPrefix(&#34;swarm_project_from_&#34;)
        options = PETSc.Options()
        options.setValue(&#34;swarm_project_from_ksp_type&#34;, &#34;lsqr&#34;)
        options.setValue(&#34;swarm_project_from_ksp_rtol&#34;, 1e-17)
        options.setValue(&#34;swarm_project_from_pc_type&#34;, &#34;none&#34;)
        ksp.setFromOptions()

        rhs = meshvardm.getGlobalVec()

        M_p = self.swarm.dm.createMassMatrix(meshvardm)

        # make particle weight vector
        f = self.swarm.createGlobalVectorFromField(self.clean_name)

        # create matrix RHS vector, in this case the FEM field fhat with the coefficients vector #alpha
        M = meshvardm.createMassMatrix(meshvardm)
        with meshvar.mesh.access():
            M.multTranspose(meshvar.vec_global, rhs)

        ksp.setOperators(M_p, M_p)
        ksp.solveTranspose(rhs, f)

        self.swarm.dm.destroyGlobalVectorFromField(self.clean_name)
        meshvardm.restoreGlobalVec(rhs)
        meshvardm.destroy()
        ksp.destroy()
        M.destroy()
        M_p.destroy()

    @property
    def data(self):
        if self._data is None:
            raise RuntimeError(
                &#34;Data must be accessed via the swarm `access()` context manager.&#34;
            )
        return self._data

    @property
    def sym(self):
        return self._meshVar.sym

    @property
    def sym_1d(self):
        return self._meshVar.sym_1d

    @timing.routine_timer_decorator
    def save(
        self,
        filename: int,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential=False,
    ):
        &#34;&#34;&#34;

        Save the swarm variable to a h5 file.

        Parameters
        ----------
        filename :
            The filename of the swarm variable to save to disk.
        compression :
            Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
        compressionType :
            Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.

        &#34;&#34;&#34;
        if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
            warnings.warn(
                &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
                stacklevel=2,
            )
        if compression == True and comm.rank == 0:
            warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)
        if filename.endswith(&#34;.h5&#34;) == False:
            raise RuntimeError(&#34;The filename must end with .h5&#34;)

        if h5py.h5.get_config().mpi == True and not force_sequential:
            with h5py.File(
                f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=MPI.COMM_WORLD
            ) as h5f:
                with self.swarm.access(self):
                    if compression == True:
                        h5f.create_dataset(
                            &#34;data&#34;, data=self.data[:], compression=compressionType
                        )
                    else:
                        h5f.create_dataset(&#34;data&#34;, data=self.data[:])
        else:
            with self.swarm.access(self):
                if comm.rank == 0:
                    # print(f&#39;start {self.name} on {comm.rank}&#39;)
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                        if compression == True:
                            h5f.create_dataset(
                                &#34;data&#34;,
                                data=self.data[:],
                                chunks=True,
                                maxshape=(None, self.data.shape[1]),
                                compression=compressionType,
                            )
                        else:
                            h5f.create_dataset(
                                &#34;data&#34;,
                                data=self.data[:],
                                chunks=True,
                                maxshape=(None, self.data.shape[1]),
                            )
                    # print(f&#39;finish {self.name} on {comm.rank}&#39;)
                comm.barrier()
                for proc in range(1, comm.size):
                    if comm.rank == proc:
                        # print(f&#39;start {self.name} on {comm.rank}&#39;)
                        with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                            h5f[&#34;data&#34;].resize(
                                (h5f[&#34;data&#34;].shape[0] + self.data.shape[0]), axis=0
                            )
                            h5f[&#34;data&#34;][-self.data.shape[0] :] = self.data[:]
                        # print(f&#39;finish {self.name} on {comm.rank}&#39;)
                    comm.barrier()
                comm.barrier()

        return

    @timing.routine_timer_decorator
    def simple_save(self, filename: str):

        # if not proxied, nothing to do. return.
        if not self._meshVar:
            if uw.mpi.rank == 0:
                print(&#34;No proxy mesh variable that can be saved&#34;, flush=True)
            return

        self._meshVar.simple_save(filename)

        return

    @timing.routine_timer_decorator
    def load(
        self,
        filename: str,
        swarmFilename: str,
    ):
        ### open up file with coords on all procs and open up data on all procs. May be problematic for large problems.
        with h5py.File(f&#34;{filename}&#34;, &#34;r&#34;) as h5f_data, h5py.File(
            f&#34;{swarmFilename}&#34;, &#34;r&#34;
        ) as h5f_swarm:
            with self.swarm.access(self):
                var_dtype = self.data.dtype
                file_dtype = h5f_data[&#34;data&#34;][:].dtype
                file_length = h5f_data[&#34;data&#34;][:].shape[0]

                if var_dtype != file_dtype:
                    if comm.rank == 0:
                        warnings.warn(
                            f&#34;{os.path.basename(filename)} dtype ({file_dtype}) does not match {self.name} swarm variable dtype ({var_dtype}) which may result in a loss of data.&#34;,
                            stacklevel=2,
                        )

                # First work out which are local points and ignore the rest
                # This might help speed up the load by dropping lots of particles

                all_coords = h5f_swarm[&#34;coordinates&#34;][()]
                all_data = h5f_data[&#34;data&#34;][()]

                cell = self.swarm.mesh.get_closest_local_cells(all_coords)
                local = np.where(cell &gt;= 0)[0]
                # not_not_local = np.where(cell == -1)[0]

                local_coords = all_coords[local]
                local_data = all_data[local]

                kdt = uw.kdtree.KDTree(local_coords)

                self.data[:] = kdt.rbf_interpolator_local(
                    self.swarm.data, local_data, nnn=1
                )

                #### this produces a shape mismatch, would be quicker not to do it in a loop
                # ind = np.isin(coordinates, self.swarm.data).all(axis=1)
                # # self.data[:] = data[ind]

                # print(f&#34;Looping over coords for swarm load&#34;)
                # i = 0

                ### loops through the coords of the swarm to load the data
                # for coord in self.swarm.data:
                #     ind_data = np.isin(h5f_swarm[&#34;coordinates&#34;][:], coord).all(axis=1)
                #     ind_swarm = np.isin(self.swarm.data, coord).all(axis=1)
                #     self.data[ind_swarm] = h5f_data[&#34;data&#34;][:][ind_data]
                #     i += 1
                #     if i % 1000 == 0:
                #         print(
                #             f&#34;Looping over coords for swarm load ... {i}/{self.swarm.data.shape[0]}&#34;
                #         )

                # print(f&#34;Looping over coords for swarm load ... done&#34;)

                ### loops through the coords in the file
                # for i in range(0, file_length):
                #     coord = h5f_swarm[&#34;coordinates&#34;][i]
                #     data = h5f_data[&#34;data&#34;][i]
                #     ind_swarm = np.isin(self.swarm.data, coord).all(axis=1)

                #     self.data[ind_swarm] = data

        return</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>underworld3.utilities._api_tools.Stateful</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="underworld3.swarm.IndexSwarmVariable" href="#underworld3.swarm.IndexSwarmVariable">IndexSwarmVariable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="underworld3.swarm.SwarmVariable.data"><code class="name">var <span class="ident">data</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def data(self):
    if self._data is None:
        raise RuntimeError(
            &#34;Data must be accessed via the swarm `access()` context manager.&#34;
        )
    return self._data</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.SwarmVariable.sym"><code class="name">var <span class="ident">sym</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sym(self):
    return self._meshVar.sym</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.SwarmVariable.sym_1d"><code class="name">var <span class="ident">sym_1d</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sym_1d(self):
    return self._meshVar.sym_1d</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="underworld3.swarm.SwarmVariable.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename: str, swarmFilename: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def load(
    self,
    filename: str,
    swarmFilename: str,
):
    ### open up file with coords on all procs and open up data on all procs. May be problematic for large problems.
    with h5py.File(f&#34;{filename}&#34;, &#34;r&#34;) as h5f_data, h5py.File(
        f&#34;{swarmFilename}&#34;, &#34;r&#34;
    ) as h5f_swarm:
        with self.swarm.access(self):
            var_dtype = self.data.dtype
            file_dtype = h5f_data[&#34;data&#34;][:].dtype
            file_length = h5f_data[&#34;data&#34;][:].shape[0]

            if var_dtype != file_dtype:
                if comm.rank == 0:
                    warnings.warn(
                        f&#34;{os.path.basename(filename)} dtype ({file_dtype}) does not match {self.name} swarm variable dtype ({var_dtype}) which may result in a loss of data.&#34;,
                        stacklevel=2,
                    )

            # First work out which are local points and ignore the rest
            # This might help speed up the load by dropping lots of particles

            all_coords = h5f_swarm[&#34;coordinates&#34;][()]
            all_data = h5f_data[&#34;data&#34;][()]

            cell = self.swarm.mesh.get_closest_local_cells(all_coords)
            local = np.where(cell &gt;= 0)[0]
            # not_not_local = np.where(cell == -1)[0]

            local_coords = all_coords[local]
            local_data = all_data[local]

            kdt = uw.kdtree.KDTree(local_coords)

            self.data[:] = kdt.rbf_interpolator_local(
                self.swarm.data, local_data, nnn=1
            )

            #### this produces a shape mismatch, would be quicker not to do it in a loop
            # ind = np.isin(coordinates, self.swarm.data).all(axis=1)
            # # self.data[:] = data[ind]

            # print(f&#34;Looping over coords for swarm load&#34;)
            # i = 0

            ### loops through the coords of the swarm to load the data
            # for coord in self.swarm.data:
            #     ind_data = np.isin(h5f_swarm[&#34;coordinates&#34;][:], coord).all(axis=1)
            #     ind_swarm = np.isin(self.swarm.data, coord).all(axis=1)
            #     self.data[ind_swarm] = h5f_data[&#34;data&#34;][:][ind_data]
            #     i += 1
            #     if i % 1000 == 0:
            #         print(
            #             f&#34;Looping over coords for swarm load ... {i}/{self.swarm.data.shape[0]}&#34;
            #         )

            # print(f&#34;Looping over coords for swarm load ... done&#34;)

            ### loops through the coords in the file
            # for i in range(0, file_length):
            #     coord = h5f_swarm[&#34;coordinates&#34;][i]
            #     data = h5f_data[&#34;data&#34;][i]
            #     ind_swarm = np.isin(self.swarm.data, coord).all(axis=1)

            #     self.data[ind_swarm] = data

    return</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.SwarmVariable.project_from"><code class="name flex">
<span>def <span class="ident">project_from</span></span>(<span>self, meshvar)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def project_from(self, meshvar):
    # use method found in
    # /tmp/petsc-build/petsc/src/dm/impls/swarm/tests/ex2.c
    # to project from fields to particles

    self.swarm.mesh.dm.clearDS()
    self.swarm.mesh.dm.createDS()

    meshdm = meshvar.mesh.dm
    fields = meshvar.field_id
    _, meshvardm = meshdm.createSubDM(fields)

    ksp = PETSc.KSP().create()
    ksp.setOptionsPrefix(&#34;swarm_project_from_&#34;)
    options = PETSc.Options()
    options.setValue(&#34;swarm_project_from_ksp_type&#34;, &#34;lsqr&#34;)
    options.setValue(&#34;swarm_project_from_ksp_rtol&#34;, 1e-17)
    options.setValue(&#34;swarm_project_from_pc_type&#34;, &#34;none&#34;)
    ksp.setFromOptions()

    rhs = meshvardm.getGlobalVec()

    M_p = self.swarm.dm.createMassMatrix(meshvardm)

    # make particle weight vector
    f = self.swarm.createGlobalVectorFromField(self.clean_name)

    # create matrix RHS vector, in this case the FEM field fhat with the coefficients vector #alpha
    M = meshvardm.createMassMatrix(meshvardm)
    with meshvar.mesh.access():
        M.multTranspose(meshvar.vec_global, rhs)

    ksp.setOperators(M_p, M_p)
    ksp.solveTranspose(rhs, f)

    self.swarm.dm.destroyGlobalVectorFromField(self.clean_name)
    meshvardm.restoreGlobalVec(rhs)
    meshvardm.destroy()
    ksp.destroy()
    M.destroy()
    M_p.destroy()</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.SwarmVariable.rbf_interpolate"><code class="name flex">
<span>def <span class="ident">rbf_interpolate</span></span>(<span>self, new_coords, verbose=False, nnn=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rbf_interpolate(self, new_coords, verbose=False, nnn=None):

    # An inverse-distance mapping is quite robust here ... as long
    # as long we take care of the case where some nodes coincide (likely if used mesh2mesh)
    # We try to eliminate contributions from recently remeshed particles

    import numpy as np

    if nnn is None:
        nnn = self.swarm.mesh.dim + 1

    with self.swarm.access():
        if self.swarm.recycle_rate &gt; 1:
            not_remeshed = self.swarm._remeshed.data[:, 0] != 0
            D = self.data[not_remeshed].copy()

            kdt = uw.kdtree.KDTree(
                self.swarm.particle_coordinates.data[not_remeshed, :]
            )
        else:
            D = self.data.copy()
            kdt = uw.kdtree.KDTree(self.swarm.particle_coordinates.data[:, :])

        kdt.build_index()

    return kdt.rbf_interpolator_local(new_coords, D, nnn, verbose)</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.SwarmVariable.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filename: int, compression: Optional[bool] = False, compressionType: Optional[str] = 'gzip', force_sequential=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the swarm variable to a h5 file.</p>
<h2 id="parameters">Parameters</h2>
<p>filename :
The filename of the swarm variable to save to disk.
compression :
Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
compressionType :
Type of compression to use, 'gzip' and 'lzf' supported. 'gzip' is default. Compression also needs to be set to 'True'.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def save(
    self,
    filename: int,
    compression: Optional[bool] = False,
    compressionType: Optional[str] = &#34;gzip&#34;,
    force_sequential=False,
):
    &#34;&#34;&#34;

    Save the swarm variable to a h5 file.

    Parameters
    ----------
    filename :
        The filename of the swarm variable to save to disk.
    compression :
        Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
    compressionType :
        Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.

    &#34;&#34;&#34;
    if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
        warnings.warn(
            &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
            stacklevel=2,
        )
    if compression == True and comm.rank == 0:
        warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)
    if filename.endswith(&#34;.h5&#34;) == False:
        raise RuntimeError(&#34;The filename must end with .h5&#34;)

    if h5py.h5.get_config().mpi == True and not force_sequential:
        with h5py.File(
            f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=MPI.COMM_WORLD
        ) as h5f:
            with self.swarm.access(self):
                if compression == True:
                    h5f.create_dataset(
                        &#34;data&#34;, data=self.data[:], compression=compressionType
                    )
                else:
                    h5f.create_dataset(&#34;data&#34;, data=self.data[:])
    else:
        with self.swarm.access(self):
            if comm.rank == 0:
                # print(f&#39;start {self.name} on {comm.rank}&#39;)
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                    if compression == True:
                        h5f.create_dataset(
                            &#34;data&#34;,
                            data=self.data[:],
                            chunks=True,
                            maxshape=(None, self.data.shape[1]),
                            compression=compressionType,
                        )
                    else:
                        h5f.create_dataset(
                            &#34;data&#34;,
                            data=self.data[:],
                            chunks=True,
                            maxshape=(None, self.data.shape[1]),
                        )
                # print(f&#39;finish {self.name} on {comm.rank}&#39;)
            comm.barrier()
            for proc in range(1, comm.size):
                if comm.rank == proc:
                    # print(f&#39;start {self.name} on {comm.rank}&#39;)
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                        h5f[&#34;data&#34;].resize(
                            (h5f[&#34;data&#34;].shape[0] + self.data.shape[0]), axis=0
                        )
                        h5f[&#34;data&#34;][-self.data.shape[0] :] = self.data[:]
                    # print(f&#39;finish {self.name} on {comm.rank}&#39;)
                comm.barrier()
            comm.barrier()

    return</code></pre>
</details>
</dd>
<dt id="underworld3.swarm.SwarmVariable.simple_save"><code class="name flex">
<span>def <span class="ident">simple_save</span></span>(<span>self, filename: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def simple_save(self, filename: str):

    # if not proxied, nothing to do. return.
    if not self._meshVar:
        if uw.mpi.rank == 0:
            print(&#34;No proxy mesh variable that can be saved&#34;, flush=True)
        return

    self._meshVar.simple_save(filename)

    return</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="underworld3" href="index.html">underworld3</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="underworld3.swarm.IndexSwarmVariable" href="#underworld3.swarm.IndexSwarmVariable">IndexSwarmVariable</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.IndexSwarmVariable.sym" href="#underworld3.swarm.IndexSwarmVariable.sym">sym</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.Swarm" href="#underworld3.swarm.Swarm">Swarm</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.Swarm.access" href="#underworld3.swarm.Swarm.access">access</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.add_particles_with_coordinates" href="#underworld3.swarm.Swarm.add_particles_with_coordinates">add_particles_with_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.add_variable" href="#underworld3.swarm.Swarm.add_variable">add_variable</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.advection" href="#underworld3.swarm.Swarm.advection">advection</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.data" href="#underworld3.swarm.Swarm.data">data</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.instances" href="#underworld3.swarm.Swarm.instances">instances</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.load" href="#underworld3.swarm.Swarm.load">load</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.mesh" href="#underworld3.swarm.Swarm.mesh">mesh</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.particle_cellid" href="#underworld3.swarm.Swarm.particle_cellid">particle_cellid</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.particle_coordinates" href="#underworld3.swarm.Swarm.particle_coordinates">particle_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.petsc_save_checkpoint" href="#underworld3.swarm.Swarm.petsc_save_checkpoint">petsc_save_checkpoint</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.populate" href="#underworld3.swarm.Swarm.populate">populate</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.save" href="#underworld3.swarm.Swarm.save">save</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.save_checkpoint" href="#underworld3.swarm.Swarm.save_checkpoint">save_checkpoint</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.vars" href="#underworld3.swarm.Swarm.vars">vars</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.SwarmPICLayout" href="#underworld3.swarm.SwarmPICLayout">SwarmPICLayout</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.SwarmPICLayout.GAUSS" href="#underworld3.swarm.SwarmPICLayout.GAUSS">GAUSS</a></code></li>
<li><code><a title="underworld3.swarm.SwarmPICLayout.REGULAR" href="#underworld3.swarm.SwarmPICLayout.REGULAR">REGULAR</a></code></li>
<li><code><a title="underworld3.swarm.SwarmPICLayout.SUBDIVISION" href="#underworld3.swarm.SwarmPICLayout.SUBDIVISION">SUBDIVISION</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.SwarmType" href="#underworld3.swarm.SwarmType">SwarmType</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.SwarmType.DMSWARM_PIC" href="#underworld3.swarm.SwarmType.DMSWARM_PIC">DMSWARM_PIC</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></code></h4>
<ul class="two-column">
<li><code><a title="underworld3.swarm.SwarmVariable.data" href="#underworld3.swarm.SwarmVariable.data">data</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.load" href="#underworld3.swarm.SwarmVariable.load">load</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.project_from" href="#underworld3.swarm.SwarmVariable.project_from">project_from</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.rbf_interpolate" href="#underworld3.swarm.SwarmVariable.rbf_interpolate">rbf_interpolate</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.save" href="#underworld3.swarm.SwarmVariable.save">save</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.simple_save" href="#underworld3.swarm.SwarmVariable.simple_save">simple_save</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.sym" href="#underworld3.swarm.SwarmVariable.sym">sym</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.sym_1d" href="#underworld3.swarm.SwarmVariable.sym_1d">sym_1d</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>